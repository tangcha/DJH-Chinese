<sidebar>
<title>You Don't Need New Data to Make a Scoop</title>
<?dbfo float-type="left"?>
Sometimes the data is already public and available, but no one has looked at it closely. In the case of the Associated Press's report on 4,500 pages of declassified documents describing the actions of private security contractors during the Iraq war, the material was obtained by an independent journalist over several years, using Freedom of Information requests addressed to the U.S. State Department. They scanned the paper results and uploaded them to DocumentCloud, which made it possible for us to do our comprehensive analysis.

Jonathan Stray (The Overview Project)
****

.Trawling Through Phone Logs
****
A few months ago, I wanted to parse (then-presidential candidate) Texas Gov. Rick Perry's phone logs. It was the result of a long-awaited state public-records request. The data essentially came in the form of 120-plus pages of fax-quality documents. It was an endeavour that required data entry and cleanup, followed by a WhitePages.com API to reverse-lookup the phone numbers. Mashing together names with state and federal (FEC) election data, http://www.boston.com/news/politics/articles/2011/12/06/perry_called_top_donors_from_work_phones/[we found] that Perry reached out to campaign and super PAC donors from state work phones, a frowned-upon practice that raised questions about ties between he and a ``super PAC'' working in his favor.

Jack Gillum (Associated Press)
****

.Know Your Rights
****
When you are publishing data, should you worry about copyright and other rights in data? While you should always clear all of this with your legal team, as a rule of thumb: if it is published by government you should neither ask forgiveness nor permission; if it is published by an organization that doesn't make money selling data you shouldn't have to worry too much; if it is published by a organization that does make money from selling data, then you should definitely ask permission.
Simon Rogers (The Guardian)
****

.Using FOI to Understand Spending
****
I've used FOI in couple of different ways to help cover COINS, the UK Government's biggest database of spending, budget and financial information. At the beginning of 2010 there was talk from George Osborne that if he became Chancellor he would release the COINS database to facilitate greater transparency in the Treasury. At this time it seemed a good idea to investigate the data in and structure of COINS so I sent a few FOI requests, one for the http://www.whatdotheyknow.com/request/schema_for_coins_database[schema of the database] one for the http://www.whatdotheyknow.com/request/training_materials_for_coins[guidance] Treasury workers receive when they work with COINS, and one for the Treasury http://www.whatdotheyknow.com/request/request_for_the_contract_with_in[contract]
with the database provider. All of which resulted in publication of useful data. I also requested all the http://www.whatdotheyknow.com/request/programme_objects_from_the_coins#incoming-78133[spending codes] in the database, which was also published. All of this helped to understand COINS when George Osborne became chancellor in May 2010 and published COINS in June 2010. The COINs data was used in a number of websites encouraging the public to investigate the data -- including http://openspending.org/[OpenSpending.org] and the Guardian's http://coins.guardian.co.uk/coins-explorer/search[Coins Data Explorer]. 

After further investigation it seemed that a large part of the database was missing: the Whole of Government Accounts (WGA) which is 1,500 sets of accounts for public funded bodies. I used FOI to http://www.whatdotheyknow.com/request/whole_of_government_accounts_200[request the 2008/09 WGA data] but to no avail. I also asked for the report from the audit office for WGA -- which I hoped would explain the reasons the WGA was not in a suitable state to be released. That was http://www.whatdotheyknow.com/request/whole_of_government_accounts_dry#comment-13231[also refused].

In December 2011 the WGA was released in the COINS data. However I wanted to make sure there was enough guidance to create the complete set of accounts for each of the 1,500 bodies included in the WGA exercise. This brings me on to the second way I used FOI: to ensure the data released under the UK transparency agenda is well explained and contains what it should. I put in a FOI request for the http://www.whatdotheyknow.com/request/cash_flow_and_balance_sheet_for#outgoing-186582[full set of accounts] for every public body included in WGA.

Lisa Evans (The Guardian)
****

.Getting Data from Paper Archives
****
Right after Wikileak's release of US military documents in Afghanistan and Iraq, we decided to adapt the concept to celebrate the 50th anniversary of the Algerian War by publishing the Algerian War Diaries. We set out to collect and digitise the archives of the French Army in Algeria. These are available in the War ministry archive in Paris, albeit in a paper format. We sent out journalists and students to take photographs of the documents. We tried scanning them using a Canon P-150 portable scanner, but it didn't work mainly because much of the archives are stapled.

In the end, about 10,000 pages were collected in a few weeks. We ran a text recognition software on them (Abbyy fine reader), which produced poor results. What's more, the ministry arbitrarily denied access to the most interesting boxes of archives. Above all, the ministry forbids anyone from republishing documents that can be freely photographed on location, so we decided that it wasn't worth the risk and the project was put on hold.
****

.When The Law Fails
****
After reading a http://kuafu.umd.edu/~ginger/research/JEH-final.pdf[scholarly article] explaining that publishing the outcome of hygiene inspections in restaurants reduced the number of food-related illnesses in Los Angeles, I asked the Parisian hygiene services for the list of inspections. Following the procedure set out by the French FOIA, I waited 30 days for their refusal to answer, then went to the Access to Public Data Commission (CADA in French), which rules on the legitimacy of FOI requests. CADA upheld my demand and ordered the administration to release the data. The administration subsequently asked for 2-month's extra time, and CADA accepted that. Two months later, the administration still had done nothing.

I tried to get some big-name (and big-pocketed) open data advocates to go to court (which is a €5000 affair and a sure win with such CADA support), but they were afraid to compromise their relations with official open data programs. This example is one among several where the French administration simply ignores the law and the official initiatives do nothing to support grassroots demands for data.

Nicolas Kayser-Bril (Journalism++)
****


.Scraping A Public Database
****
Some French physicians are free to choose their own rates, so that one can pay between €70 and €500 for a 30-minute visit at an oncologist, for instance. This data regarding rates is legally public, but the administration provides only a hard-to-navigate online access to the database. In order to have a good view of the doctors' rates for Le Monde, I decided to scrape the entire DB.

That's where the fun began. The front-end search form was a Flash application, that redirected to an HTML result page via a POST request. With help from Nicolas Kayser-Bril, it took us some time to figure out that the application used a third page as a ``hidden'' step between the search form and the result page. This page was actually used to store a cookie with values from the search form that was then accessed by the results page. It would have been hard to think of a more convoluted process, but the options of the cURL library in PHP make it easy to overcome the hurdles, once you know where they are! In the end, getting hold of the database was a 10-hour task, but it was worth it.

Alexandre Léchenet (Le Monde)
****

.FOI With Friends
****
Many Balkan countries have issues with government corruption. Corruption is often even higher when it comes to accountability of the local governments in those countries. For several months a group of Serbian journalists around the Belgrade-based http://www.cins.org.rs/[Centre for Investigative Reporting] have been questioning different types of FOI documents from over 30 local municipalities in 2009. Prior to that, almost nothing was accessible to the public. The idea was to get the original government records and to put the data in spreadsheets, to run basic checks and comparisons among the municipalities and to get maximum and minimum figures. Basic indicators were budget numbers, regular and special expenses, salaries of officials, travel expenses, numbers of employees, cell phone expenses, per diems, public procurement figures and so on. It was the first time that reporters had asked for such
information.

The result was a comprehensive database which unravels numerous phoney representations, malfeasances and corruption cases. A list of the highest paid mayors indicated that a few of them were receiving more money than the Serbian president. Many other officials were overpaid, with many receiving enormous travel repayments and per diems. Our hard earned public procurement data helped to highlight an official mess. More than 150 stories came out of the database and many of them were picked up by the local and national media in Serbia.

We learned that comparing the records with the comparable data from similar government
entities can display deviations and shed light on probable corruption. Exaggerated and unusual expenses can be detected only by comparing.

Djordje Padejski (Knight Journalism Fellow, Stanford University)
****

.Dirty Data
****
Thanks to the generally strong public records laws in the United States, getting data here isn't as big a problem as it can be in many other countries. But once we get it, we still face the problems of working with data that has been gathered for bureaucratic reasons, not for analytic reasons. The data often is ``dirty'', with values that aren't standardised. Several times I have received data that doesn't match up to the supposed file layout and data dictionary that accompanies it. Some agencies will insist on giving you the data in awkward formats like .pdf, which have to be converted. Problems like these make you appreciate it when you do get an occasional no-hassle dataset.

Steve Doig (Walter Cronkite School of Journalism of Arizona State University
****

.Patching, Scraping, Compiling, Cleaning
****
The challenge with huge swathes of UK data isn't getting it released -- it's getting it into a usable format. Lots of data on hospitality, MPs' outside interests, lobbying and more is routinely published but in difficult-to-analyze ways.

For some information, there is only the hard slog: patching together dozens of excel files each containing just a dozen or so records was the only way to make comprehensive lists of ministerial meetings. But for other information, web scraping proved incredibly helpful.

Using a service like ScraperWiki to ask coders to produce a scraper for information like the Register of MPs' interest did around half of our job for us: we had all MPs' information in one sheet, ready for the (lengthy) task of analysing and cleaning.

Services like this (or tools such as Outwit Hub) are a huge help to journalists trying to compile messy data who are unable to code themselves.

James Ball (The Guardian)
****

.Mixed Up, Hidden and Absent Data
****
I remember a funny situation where we tried to access the Hungarian data on EU farm subsidies: it was all there – but in an excessively heavy PDF document and mixed up with data on national farm subsidies. Our programmers had to work for _hours_, before the data was useful.

We also had a pretty interesting time with data about EU fish subsidies, which national payment agencies in all 27 Member States are obliged to disclose. Here's an excerpt from http://brigittealfter11.files.wordpress.com/2012/02/2009-slipping-through-the-net.pdf[a report we wrote on the topic]: ``In the United Kingdom, for example, the format of the data varies from very user-friendly HTML search pages to PDF overviews or even lists of recipients in varying formats hidden away at the bottom of press releases. All this is within just one member state. In Germany and Bulgaria, meanwhile, empty lists are published. The appropriate headings are there but without any data.''
Brigitte Alfter (Journalismfund.eu)
****

.Going Straight to the Source
****
The first trick I use in getting a hold of data that is held by a public entity is to try to go directly to the data-holder, not the public affairs person, not through an FOIA. I could craft an FOIA or public records request of course, but it will start the wheels turning slowly. It's likely that I'll get a response back that the data is not in the format I requested, or (as has happened in some cases) the governmental body uses a proprietary software and can't extract the data in the format I requested. But, if I first successfully reach the person who handles data for that organization, I can ask questions about what data they keep on the subject and how they keep it. I can find out the format. I can speak data-language and find out what I need to know to successfully request the data. The roadblocks to this approach? Often, it's hard to reach these people. The Public Information Officer (PIO) will want me to deal with them. I've found in those cases, it's best to try to then set up a conference call, or even better, an in-person meeting with the PIO, the data guru and me. And I can set it up in a way that makes it hard for them to say no. ``I don't want to create work for them'' I say. ``I don't want to create an unnecessarily burdensome or overly broad request, so a meeting will help me understand exactly what they have and how I can best request exactly what is needed.''

If this method does not work, my fallback is to ask first for their record layout and data dictionary in a request. Then, I actually request the data. I sometimes will also ask first how they keep the data, and in what system. That way, I can research the ways the data can be exported in advance of writing my request.

Lastly, my best success story comes from when I was working at a small newspaper in Montana. I needed some county data, which I was told could not be exported out of the mainframe. I did a little research, offered to come in and help. Worked with the data person, we built a short script, printed the data to a (this was a long time ago) floppy disk. I had my data and the county now was equipped to provide data to anyone who requested it. They didn't intend for that to happen, but they needed to extract the data sometimes too and didn't fully understand their system, so we all helped each other.

Cheryl Philips (Seattle Times)
****

.The 65k Rule
****
Upon receiving the first dump of Afghan war log data from Wikileaks, the team processing it started talking about how excited they were to have access to 65k military records.

This immediately set alarms ringing amongst those with experience of the Microsoft Excel. Thanks to an historic limitation in the way that rows are addressed, the Excel import tool won't process more than 65,536 records. In this case, it emerged that a mere 25k rows were missing!

The moral of this story (aside from avoiding using Excel for such tasks), is to always be suspicious of anyone boasting about sixty five thousand rows of data.

Alastair Dant (The Guardian)
****
