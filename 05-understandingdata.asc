:chapnum: 05
:figure-number: 00

== 理解数据

image::figs/incoming/05-00-cover.png[float="none",role="informal"]

++++
<?dbfo-need height="1in"?>
++++

一旦你有了数据，那你应该对这些数据做什么呢？你应当寻找什么？你应该使用哪些工具？这一部分以一些能提高你数据素养的意见，处理数字和统计的小提醒，以及在处理复杂棘手，经常有缺失数据时候需要牢牢记住的东西开始，接着讲述学习如何从数据中讲故事，数据新闻记者对工具的选择还有如何使用数据可视化提供你关心话题的洞察。

=== 简单三步让自己变的有数据素养

正如文字素养着重于``通过阅读获取知识，能条理写作，并可以批判性分析书面材料的能力''，数据素养是一种消化数据获取知识、梳理并批判性分析数据的能力。数据素养不仅包括统计素养，更需要懂得如何处理庞大的数据集，明白这些数据集是怎样产生的，知道怎样把各种数据集联系起来，且懂得解释它们。

[[FIG051]]
.http://www.flickr.com/photos/jdhancock/3386035827/[深入挖掘数据] (JDHancock 摄)
image::figs/incoming/05-MM.jpg[float="none"]

++++
<?dbfo-need height="1in"?>
++++

波音特新闻大学(Poynter's News University)开设了针对 http://www.newsu.org/courses/math-journalists[新闻工作者的数学课程]，帮助他们去理解诸如比例变化和平均数等概念。有趣的是，与此同时，在距离波特因学院不远的佛罗里达州的小学里也面向5年级的学生（10-11岁的孩子）开设涵盖同样知识的 http://bit.ly/k12-courses[课程作为必修课]。

这些新闻从业者急需的数学知识竟然来自高中之前的课程，可见如今新闻编辑部的数据素养有多欠缺。这是个大问题。如果一个数据新闻从业者连什么是“置信区间”都不知道，怎么去利用全球气候变化的系列数据呢？如果一个数据新闻记者连 http://bit.ly/karenberger-mean-median[中位数和平均数都无法区分]，怎么写关于收入分配的报道？

当然，一个记者不需要为了更高效率地处理数据而去专门拿一个统计学的学位。不过，如果掌握一些数据处理的小技巧的话，那么，他们面对数字的时候会从中挖掘出更有价值的信息来，进而写出更为出色的报道。正如马克思·普朗克学院教授 http://bit.ly/ddjnet-numeracy[杰德·吉仁泽所说](Gerd Gigerenzer)，如果缺少洞察，那么再好的数据处理工具也无助于新闻质量的提升。

所以，接下来，你只需要问三个简单的问题，即使你在数学或者统计学知识方面有所欠缺你也可以成为一名老练的数据记者。

==== 1. 数据是怎么被收集的？

===== 惊人的GDP增长

伪造数据是利用重大数据出风头的捷径。这听起来毫不稀奇，但正如GDP数据通常被人们评论的那样，数据很可能是假的。前英国大使卡瑞吉·默里在其著作 http://amzn.to/murder-samarkand[《撒马尔罕城谋杀》]中称，乌兹别克斯坦的经济增长率受制于地方政府和国际经济体之间的紧张谈判。换句话说，它与地方经济没有任何关系。

GDP被作为首要经济发展情况参考指标是因为政府需要用它来监控自己的主要收入来源——增值税。一旦当一个政府不靠增值税提供资金，或当它不公布财政预算时，就没有采集GDP数据的理由了，并且通过伪造GDP数据，国家会看起来富足繁荣。

===== 犯罪率永远在增加

http://bit.ly/elpais-numeracy[《国度报》报道称]，``西班牙的犯罪率上升了3%。'' http://bit.ly/rtl-numeracy[RTL电视台说]，布鲁塞尔正在打击非法侨民和吸毒者犯罪。这类基于警方统计资料的报道很常见，但关于犯罪的更多情况，它没有反映出来。

我们可以相信，在欧盟内部，数据没有被篡改。但警方应对犯罪发生的诱因做出更多回应。比如，当工作业绩与打击犯罪率挂钩时，警察就被鼓励尽可能多地汇报不需要调查的犯罪事件。这类罪行之一就是吸毒。从而，这就解释了为什么在法国与毒品相关的犯罪在过去15年中翻了四倍，但毒品消耗量却不变。

===== 你能做些什么？

当怀疑一个数字的可信度时，往往要进行反复检查，就像你寻找数据时那样——就算它引用自官方。在乌兹别克事件中，给在当地居民打个电话就完全足以证实数据的可信性了（“问问该国是不是像官方数字显示的（那样），（感觉像是）比1995年时富裕了3倍”）。

针对警方数据的可信性，社会学家常常会进行受害者研究。在这个过程中，他们会询问人们是否遭遇犯罪事件，以此来验证警方数据的真实度。这些研究所得数据要比警方数据更平稳。因此，也许这就是它们上不了头条新闻的原因。

虽然其他检验方法可以让你准确评估数据的可靠性，比如本福德法则(Benford's law)，但最重要和有效的方法还是你自己的批判性思考。

==== 2. 我们应该从中学到什么？

===== 夜里工作会使多发性硬化症的风险加倍

相信一些理智的德国人在读了本篇新闻 http://bit.ly/dmsg-numeracy[这个标题]后都会停止在夜里工作。但这篇文章最后并没有告诉我们真实的风险是什么。

1000个德国人中，只有一个会在有生之年患上多发性硬化症。假设现在，如果这1000个德国人都上夜班，那么多发性硬化症患者的数量将上升到2个。因为上班时间改变而增加的患多发性硬化症的风险是1/1000，不是100%。所以，当你在考虑是否接受一份工作时这个信息或许更有用。

===== 平均15个欧洲人里就有一个是彻底的文盲

上面的标题看起来很唬人，但是它也绝对是真实的。在50亿欧洲人中，有3,600万可能都不识字。但是，即使文盲数量达到3600万，其比例也仍然低于7%（数据来自 http://bit.ly/eurostat-numeracy[欧洲统计局]）。

在采用平均值的时候，要时刻思考``是什么的平均值？'' 涉及的基数是同质的吗？例如，非均匀分布的样本就解释了为什么大多数人的驾驶水平都高于平均值。很多人一生都没有或仅出过一次事故。但是，几个鲁莽的司机制造了大量事故，就会推高事故的平均值，使其高于大多数人的经历。对于收入分配也是同样的道理：即，大多数人的收入低于平均值。

===== 你能做些什么

随时注意数据分布和基数比率。通过检验其平均值和中位值，以及靠众数（分布中最常见的数值），来进行数据洞察分析。就像在多发性硬化症报道的例子中，分辨出哪种数据更重要，就能更容易地配合主题合理运用数据。最后可知，从本盏率的角度进行报道（1000个中有1个）比用百分比（1%）更易于读者理解。

==== 3. 信息有多可靠？

===== 样本量的问题

由 http://bit.ly/diariodenavarra[萨拉戈萨出资的<Diaro de Navarra>]公布的一份调查称``80%的人对司法系统不满意。''仅仅从4.6千万西班牙人中找了800名受访者进行调查，怎么能做出这一推断？这一数据明显被夸大了。

在对大量人口进行研究时（数以千计），要控制误差率低于3%，只需要不到1000的抽样人口。这意味着，如果你找完全不同的样本重新调查，10次中有9次，你得到的结果和你最初得到的结果之间的误差不会超过3%。统计学是很有用的东西，而且在狡猾的调查中，抽样出来的样本量几乎不会受到质询。

===== 喝茶能降低中风的风险

关于喝茶的益处的文章司空见惯。《德国世界报》上的 http://bit.ly/welt-tea[这篇短文]称，茶叶能完全地降低心肌梗塞风险。虽然一些人认真研究茶叶的功效，但很多研究没有考虑到生活方式的因素，例如减肥、消遣或运动。

在大多数国家，茶是有保健意识的上层人的饮料。如果研究者不能在对茶的研究中考虑进生活方式的因素，那么他们能告诉我们的就就只能是“富人更健康，原因可能是他们很可能喝了茶”。

===== 你能做些什么

在有关茶的报道的案例中，在很多情况下，相关性和误差背后的数学规律同样适用。但如果研究者不寻找相互关联的因素（例如喝茶与做运动的联系），他们的研究结果将毫无价值。

作为一名新闻工作者，去挑战一项研究的数值例如样本量的结果毫无意义，除非严重怀疑数据的可信性。但是，观察研究者是否考虑了相关信息则很容易做到的。

&mdash; _尼古拉斯·凯瑟－布瑞尔（Nicolas Kayser-Bril），Journalism++_

=== 新闻中的数字运用技巧

* 处理数据的最佳技巧就是让自己乐在其中。数据看起来令人生畏，但如果被它吓住，你将一无所获。如果像做游戏和探险一样对待它，那么，它将变得格外容易地透露秘密和真相。所以，就像你处理其它证据一样简单地处理它吧，不用害怕也不用另眼相看，甚至可以将之当成一次想象力训练。在选择报道角度时，不妨更具创造性一点，使之能更符合数据、更好地解读数据，然后再用更多论据去印证它。“其它报道会怎样解读？”是个简便的参考方法，帮助你思考这个数字——这个明显偏大或不正常的数字、明显反映出这种或那种状态的证据，怎么会被解读出截然不同的结果。

* 不要把对数据的质疑和轻视搞混淆。质疑是好的，轻视却是粗暴地放弃数据。如果你相信数据新闻，不管你是否阅读本书，你都必须相信，数据能提供的东西，远远好于那些讽刺文章中的谎言和胡言乱语，以及倾向性报道中经过筛选，意图诋毁某人的客观事实。如果好好利用，数据常常能带给我们深刻的认识。所以，我们在运用它时既不能轻视也不能轻信，而需要慎重。

* 如果我告诉你经济衰退时饮酒率上升，你可能会说那是因为人人都心情低落；可如果我告诉你饮酒率下降，你可能会说那是因为人人都经济窘迫。换句话说，数据说什么，不影响你已经认定了的理由（诠释），也就是说，你通过不同方式，总可以证明情况很糟糕。这里要提出的观点是，如果你相信数据有用，试着在你凭着情绪、信仰和预期展开辩论前，先让它说话。数据那么多，只要你随便找一找，基本都能发现足以印证你之前观点的证据。换句话说，如果你不够虚心，至少在我看来，数据新闻对你的帮助不大。你想做的只是达到你的目的，而非解读出基于数字（之）上的事件本质。

* 数据新闻的结论不确定是正常的。因为，我们习惯于将数据看作权威的和确定的，但事实常常并非如此。答案是没有答案，或者答案是我们能得到的最好的但仍不能保证绝对正确的。我想我们应该面对这些事实。如果你认为它听起来像是毁掉报道的好方法，那我要告诉你，其实它恰恰是一个发现新问题的好方法。同样的，常常有多种裁剪数据的合理方法。数字并不是只有对或错两种情况。

* 调查过程就是一篇报道。在你逐个寻找证据时，你如何努力发掘真相的经历就能做一篇好新闻——这种寻找肯定有助于从数据中获取证据，因为仅仅一个数字几乎不能形成证据。不同的消息源能提供全新的角度、全新的构想，更多元的理解。我想我们是太急于成为权威并告诉人们答案了——所以，我们因为没有公布调查过程而失去了一个做出好新闻的诀窍。

++++
<?dbfo-need height="1in"?>
++++

* 最好的问题还是老问题：这个数字真的重要吗？从哪里能够获得？你确定它真的如你认为的一样有价值？这些通常只是提醒你全盘考虑数据，只看单个数字会使眼光狭隘。现实生活环境、将数据作纵向对比时覆盖的时间长度、数据的分类和结构...简而言之，所有关于数据的背景，都要考虑。

&mdash; _迈克尔·布拉斯兰(Michael Blastland)，自由记者_

=== 处理数据的基本步骤

在对数据进行处理之前，你至少需要知道以下三个关键点::

* 数据需求的提出取决于你想要解决的问题。

* 数据通常是不规范的，在使用之前需要进行清洗。

* 数据可能包含未记录的内容。

下文将详细展开这三点：

[[FIG052]]
.杂乱的数据
image::figs/incoming/05-MM.png[scale="89",float="none"]

==== 了解你想弄清楚的问题

在很多时候，与数据打交道跟采访（现场）信息源一样，你需要通过挖掘跟数据相关的问题来揭示答案。但正如同信息源告诉你的都是他或她所知道的与之相关的信息，数据也只有在被正确地记录、拥有恰当的变量的变量的情况下才能回答你提出的问题。这意味着，在获取数据之前你就需要仔细考虑清楚你想要通过数据来回答什么问题。因此，通常你的数据处理工作是回溯性的。首先，列出你想在报道中通过数据来证明的论点。然后，为了证明这些论述，确定该获取、分析哪些变量和数据记录。

以当地犯罪报道为例子。比方说，你想写一篇揭示所在城市犯罪模式的报道，那么就会涉及到在一天中的哪个时段，或者是在一周中的哪几天最容易发生犯罪行为；又或者，哪些地方是各种犯罪行为的“热点”区域。

那么，你会意识到，你需要的数据包括（报导中每次）犯罪的日期和时间、犯罪的类型（杀人，盗窃，爆窃等）以及犯罪发生的地点。因此，为了回答你所提出的问题，你至少需要日期、时间、犯罪类型和地点这四个变量。

不过，需要注意的是，如果数据集只包括这四个变量，一些潜在的有趣问题就_无法_得到回答。比如说，受害者的性别与种族、被盗窃财产的总价值，以及哪位警官逮捕罪犯最卓有成效等。此外，你获得的相关数据记录可能只涵盖一段时间，比如说过去三年。这意味着你没法说明犯罪模式在一个更长的时间段里是否发生了变化。这些问题可能并不在你原本的报道计划当中，那么，这就没有问题。但是，你一定不会希望当你一股脑扎进数据分析之后，突然决定要知道这些不在计划当中的数据，例如，在不同的地区，犯罪分子被绳之于法的比例是多少的情况发生的。

这里的经验教训是，数据需求应当是包括数据库中所有变量和记录的_完整_数据，而不是为了解决当下报道中需要回答的问题而获取的子数据集。（事实上，如果你需要花钱来获得所需要的数据的话，那么获取完整数据往往比只要一个子数据集便宜。）你可以随时从数据中截取需要的那部分，而获得完整的数据后，还可以帮助你回答报导中可能遇到的新问题；你甚至还可能从中为后续报道找到新点子。虽然，某些信息，比如受害人的身份或是秘密线人的姓名，可能会因为保密法或者其他法规而无法公开。但在报道当中即使是部分的数据呈现也远远比没有好，只要你明白哪些问题是通过数据分析可以回答的，而哪些不能。

==== 清洗数据

数据库工作中最大问题之一是，你需要将出于行政需求收集的数据拿来做分析使用，可问题是，这两类数据的精确标准大不一样。

例如，犯罪司法系统数据库的一个重要的作用是轮到被告人琼斯被听证的时候，确保他能够从监狱里被带到法官史密斯面前。出于此目的，琼斯的出生日期是否准确、住址的街道名称有没有拼写错误，甚至他的中间名缩写有误，真的一点都不重要。一般情况下，该系统仍能用这份不完美记录在指定的时间把琼斯带到史密斯的法庭上。

但是，这些错误会严重影响记者试图通过数据库来发现当地犯罪模式而所做的努力。基于这个原因，当你获得一个新数据库时，首要任务是确定它到底有多凌乱，然后把它清理干净。一个有效快速的方法可以找到错误数据：创建统计绝对变量的频次表，绝对变量即那些预计变值会相对较少的变量。（如果是用Excel，你可以通过在每个绝对变量上使用筛选或者透视表来实现）。

用``性别''来举个简单例子。你会发现，你的性别栏中有各种数据值：男性、女性、男、女、1、0、男人、女人等等，甚至还有诸如“Femal”这样的错误拼写。为了做一个合理的性别分析，你需要制订一个标准——比如说用M和F来分别表示男性和女性——然后改写所有的不同写法以符合该标准。另一个常常会碰到这类问题的数据库是美国竞选财务记录。在这个数据库中，职业一栏中，（比如律师这一职业）会有诸如``Lawyer、''``Attorney、''``Atty、''``Counsel、''``Trial Lawyer''等多种多样的写法以及拼写错误。同样的，解决问题的诀窍是规范职业称呼，避免过多的变化。

处理名字的时候，数据清理的工作变得更加棘手。``约瑟夫·T·史密斯''、``约瑟·史密斯''、``J.T.·史密斯''、``约什·史密斯''是同一个人吗？这时候需要查看其它的变量，比如说地址或是出生日期。有时候甚至需要仔细研究别的数据记录才能够确定。通过使用类似于Google Refine这样的工具可以让清理和标准化的工作更加快捷，不那么劳累。

.Dirty Data
****
Thanks to the generally strong public records laws in the United States, getting data here isn't as big a problem as it can be in many other countries. But once we get it, we still face the problems of working with data that has been gathered for bureaucratic reasons, not for analytic reasons. The data often is ``dirty,'' with values that aren't standardized. Several times I have received data that doesn't match up to the supposed file layout and data dictionary that accompanies it. Some agencies will insist on giving you the data in awkward formats like .pdf, which have to be converted. Problems like these make you appreciate it when you do get an occasional no-hassle dataset.

&mdash; _Steve Doig, Walter Cronkite School of Journalism, Arizona State University_
****

==== 数据可能包含未记录的内容

任何数据库的“罗塞塔石碑”就是所谓的数据字典。一般而言，这个文件（通常是text或者PDF甚至是电子表格）会告诉你这个数据的格式（delimited text、fixed width text、Excel、dBase等），变量的顺序、各个变量的名字以及各个变量的数据类型（文本字符串、整形、浮点等）。你可以利用这些信息帮助你把数据文件恰当地将导入到你想使用的分析软件中（Excel、Access、SPSS、Fusion Tables、各种SQL等）。

数据字典中还有另一个关键元素——解释特定变量的信息使用的代码。例如，性别可能被编码，使“1 =男“和”0 =女“。罪犯们的罪行种类可能会以司法管辖区的法规号码来代替。医院治疗记录可能会用数以百计的五位数代码中任何一个来表示对受诊病人的诊断。没有数据字典，这些数据集很难、甚至不可能被正确地分析。

但是，即使数据字典在手，还是会出现问题。一个例子是，若干年前佛罗里达州的《迈阿密先驱报》记者分析因酒醉被捕的人们受到的判罚如何因不同的法官而发生变化。记者从法院系统获得定罪纪录，并根据数据字典分析了三个不同的处罚变量的数据：监禁时常（时长）、拘留时常（时长）和罚款金额。这三个数据会因法官的不同而有所变化。记者以此为证据，写了篇关于有些法官判罚严厉、有判罚些温和的报道。

但每一位法官都有约1-2％的判决数据没有显示监禁时间、拘留时间和罚款金额。因此，在展现（展示）不同（每个）法官的判决模式的图表中，都有一小部分案子显示“没有惩罚（零处罚）”，（即使在复审时）。当新闻报道和图表出现在报纸上后，法官们厉声抱怨《迈哈密先驱报》是在控告（控诉）他们违反州法律，因为根据州法律，任何人酒后驾驶都要受到惩罚。

因此，记者又回到了生产（制作）这些数据文件的法院办公室（书记员那里），询问是什么造成了这个错误。他们被告知，这些（被质疑的）案件涉及的是首次被捕的经济窘迫的被告。一般情况下，他们会被要求支付罚款，但他们没钱。所以法官判罚他们去进行社区服务，比如说清理街道上的垃圾。由此导致的结果是，数据库结构创建完成后，这些法律要求的惩罚被忽略了。因此，每一位书记员都知道，在数据中，监禁、拘留、罚款都为零意味着社区服务。然而，这_并没有_在数据字典中被标注出来，并因此造成《迈哈密先驱报》发布撰写（更正）启事。

在这种情况下（这件事）的教训是，要向给你数据的工作人员（机构）咨询，数据中是否有未记录的数据（元素），无论它是新近创建、还未被收录入数据字典的代号（代码），还是文件布局改变（编排的改动），亦或者是别的什么东西。此外，不要忘记检查你的分析结果，并问：“这有意义（讲得通）吗？”《迈哈密先驱报》的记者绘制图表已经临近截稿时间，并且他们过于专注在每名法官的平均判罚水平，没有注意到那些看上去“没有收到惩罚（零处罚）”的（容易忽略的）少数案件。他们应该问自己，如果真是如此，所有的法官（看起来）都违反了州法律（是否讲得通），哪怕仅仅是在很小的程度上。

&mdash; _史蒂夫·多伊格（Steve Doig），沃尔特·克朗凯特新闻学院，亚利桑那州立大学_

++++
<?dbfo-need height="2in"?>
++++

.Mixed Up, Hidden and Absent Data
****
I remember a funny situation where we tried to access the Hungarian data on EU farm subsidies: it was all there--but in an excessively heavy PDF document and mixed up with data on national farm subsidies. Our programmers had to work for _hours_ before the data was useful.

We also had a pretty interesting time with data about EU fish subsidies, which national payment agencies in all 27 Member States are obliged to disclose. Here's an excerpt from http://bit.ly/alfter-eu27[a report we wrote on the topic]: ``In the United Kingdom, for example, the format of the data varies from very user-friendly HTML search pages to PDF overviews or even lists of recipients in varying formats hidden away at the bottom of press releases. All this is within just one member state. In Germany and Bulgaria, meanwhile, empty lists are published. The appropriate headings are there but without any data.''

&mdash; _Brigitte Alfter, Journalismfund.eu_
****


=== 32英镑的一条面包

这是一篇周日威尔士地区报道的有关威尔士政府在无麸产品配方上的花费的新闻稿。 http://bit.ly/walesonline-gluten-free[大标题中标明]一块面包需要花费32英镑。但是，准确的情况却是11块面包，每块2.82英镑

数字来源于威尔士议会的书面答复和威尔士地区国民健康保障体系发布的有关每一配方所花费的金额这一项。但是，却没在有关数据指标的定义部分的数据字典里对数据和数据指标进行附加说明和定义。

因此，就导致人们理所当然的假设这针对的是每个个体，即是，一条面包的花费，而不是事实上一打面包的花费。

没有一个人，既包括就这个数据进行书面回复也包括新闻办公室的人，都没有发现这个问题，直到这则报道在星期一刊发出来的时候，大家才发现了数量上的问题。

所以，这个事件告诉我们，不能想当然地认为政府发布的数据中那些说明背景信息的注释能有效地把数据解释清楚，即使你告诉发布数据地人员，这样的数字表述会造成读者错误地数据解读，他们还都意识到数据的解读不够清楚。

通常来讲，报纸都希望新闻报道有一个好标题，所以，除非是在非常明显的无法解释得通的情况下。往往这些能够吸引人眼球的报道标题都很容易通过，没人会太仔细的去检查核实，特别在临近截止日期时还要冒着报道被砍掉得风险。

[[FIG053]]
.无麸质面包法令花销威尔士纳税人32英镑 (威尔士在线 WalesOnline)
image::figs/incoming/05-AA.png[float="none"]

但是，即使可能导致新闻报道被砍掉，记者也有责任去核查那些荒谬的论断。

&mdash; _克莱尔·米勒(Claire Miller)，威尔士在线(WalesOnline)_

=== 从数据开始，以故事结尾

为了吸引读者，你得用标题中的数字让读者打起精神并引起他们的注意；即便是不知道背后的数据集，你也应当可以阅读故事；要让故事激动人心，并时刻牢记哪些人是你的读者。

One example of this can be found in a project carried out by the Bureau of Investigative Journalism using the EU Commission's http://bit.ly/ec-fts[Financial Transparency System]. The story was constructed by approaching the dataset with specific queries in mind.
其中一个例子是，新闻调查局（Bureau of Investigative Journalism）实施的一个项目中，使用了欧盟委员会的 http://bit.ly/ec-fts[财务公开系统]。那么这个报道也正是来源自我们最初想在数据库中查询的一些数据。

我们利用诸如“鸡尾酒”、“高尔夫”和“假期”等关键词搜索数据。这让我们确定了委员会在这些项目上的花费，并让随后我们提出大量的问题并作报道。

但是通过关键词不是每次都能找到你要的，有时你得坐定思考你真正寻求的东西。项目进行过程中，我们还想得知委员们在私人（喷气式）飞机旅行上的花费，但数据里没有“私人喷气式飞机”这个条目，我们不得不靠其他方法得知他们旅行供应商的名字。一旦我们知道为委员会提供服务的供应商名字叫“Abelag”，我们就能通过查询数据得知由Abelag提供的服务开销是多少了。

通过这个方法，我们在查询数据时就有了定义确切的对象；找到能够支撑标题的数字，以及整个的基调。

另一种方法是从黑名单着手，查找额外项目。从数据中找到故事的简单办法就是知道有哪些东西是数据库中不应该有的。《金融时报》与新闻调查局联合的欧盟结构基金项目对此作出了很好的说明。

委员会自己制定了规则，规定了哪种类型的公司和协会应当被禁止接收结构基金。对香烟和烟草生产商的开支是其中一个例子。

以烟草公司、生产商和种植商的名字来查询数据，我们找到数据显示英美烟草集团从德国一家工厂处接收了150万欧元，这笔资金违反了委员会关于开支的规定——所以说这是个从数据中找到故事的快捷办法。

你永远不会知道自己将在数据集里得到什么讯息，所以尽管来看一眼。你需要勇敢一些，当你使用筛选工具（最大、极端、最普遍，等等）来确定一些明显的特征时，往往就能有所斩获。

&mdash; _Caelainn Barr, Citywire_

=== 用数字说话

数据新闻有时会给人一个印象，即它主要是关于数据展现的。比如数据可视化，迅速而又强大地传达对一堆数字某一方面的理解；再比如可搜索的交互式数据库，任何人都可以在里面查询比方说自己当地的街道或者医院信息等。所有这些都非常有价值，但是跟其他类型的新闻一样，数据新闻也应当是一个个的故事。那么你能在浩瀚的数据中发掘哪些故事呢？基于我在BBC的从业经历，我写了如下一个列表，或者说是各种不同类型数据故事的“类别模型”吧。

不管你是在分析数据，还是处于搜集数据这前一阶段（无论是寻找公开数据还是发起信息自由申请），我认为牢牢记住下面列出的这些都是很有帮助的。

测量::
  最精简的新闻故事；计数与求和: “去年，全国各地的地方议会总共在采购回形针上花了X万亿英镑。” 但通常这样一个笼统的数字很难让人明白到底是花多了还是花少了。因此，你得把数字放进特定的语境中——比如，可以运用：

 比例;;
 “去年，全国的地方议会在回形针上的支出占到全部文具预算的三分之二。”

国内比较;;
  “地方议会在回形针上的支出多过为空巢老人送餐到家服务上的支出。”

海外比较;;
  “去年，议会在回形针上的支出是国家海外救援预算的两倍。”

当然在特定语境下或者用比较的方法来探索数据还有其他各种各样的方式。

随时间变化::
  “四年来，议会在回形针上的开销增长了两倍。”

“排名表”::
  因为通常会有地域或惯例上的差异，所以你得确保用来做比较的基础是公平的（即要把当地人口规模的考虑进去）。“Borsetshire议会的工作人员在回形针上的人均花销要高于其他地方议会。前者的数值是全国平均水平的四倍。”

或者你可以把整个数据分成几组:

分类分析法::
  “紫党政务委员会用于购买纸夹的开销比黄党的多出50%。”

或者你可以用数字把各个因素联系在一起:

关联比较法::
  ”接受过文具用品公司捐款的那些政务委员会用在纸夹的开销更大，平均每一英磅的捐款，开销增长100英镑。“

当然，你要记住，相互关系和因果关系不是一回事。

++++
<?dbfo-need height="1in"?>
++++

因此，如果是在调查购买纸夹的开销，你是不是也获得了以下的数据？

  * 能提供语境的总支出是多少？
  * 能作为参照的各地区数据、历史记录和其他的统计数据？
  * 辅助性的数据，比如人口参数？这能保证对比的公平。
  * 其他的数据？有意思的、有联系的数据可拿来与此项开销进行对比。

&mdash; _马丁·罗森鲍姆(Martin Rosenbaum)，BBC_

=== 数据记者对工具选择的讨论 ===

噗嘶嘶嘶…这是你的数据从压缩包里解压的声音。现在怎么办？你想要从数据里寻找到什么？ 准备用什么数据处理工具？对此，我们询问了一些数据新闻记者，看他们是如何处理数据的。以下是他们的经历…

[quote, 丽莎·埃文斯(Lisa Evans), 卫报]
____
《卫报》的数据博客非常看重与读者互动，这使读者能够在我们的基础上，快速复制《卫报》的数据新闻报道，并且发现一些我们没有发现的东西。因此，越直观的数据处理工具就越好。我们尽量挑选任何人都不用学习编程语言或经过特殊训练就能掌握、并没有高额附加费用的数据处理工具。

基于这个原因，目前我们大量使用谷歌的有关数据处理的产品。我们整理和发布的所有数据集都可以通过谷歌电子表格呈现，这意味着任何有谷歌帐户的人都可以下载数据，导入到自己的帐户，制作自己的图表，对数据进行排序，并创建数据透视表，也可以将数据导入到他们所选择的工具里。

我们使用谷歌的融合表(Google Fusion tables)来组织数据。当我们在融合表中创建热力图时，也将我们的KML文件分享到网站上，这样读者可以下载并建立自己的热力图，包括在数据博客的原始图上加入新的数据层。这些谷歌工具还有一个不错的功能是，他们适用于读者访问博客的不同终端，比如台式电脑、手机和平板电脑。 

除了谷歌电子表格和融合表，我们在日常工作中还使用了其他两个工具。一是tableau，一个多维数据集可视化的工具，二是ManyEyes，用来对数据进行快速分析的工具。不过，这些工具都不够完美，所以我们将继续寻找让读者喜欢的更好的可视化工具。
____

[quote, 辛西娅·奥墨楚(Cynthia O'Murchu), 金融时报]
____
我会变成一个程序员吗？不太可能！我当然不认为每一位的记者都需要知道如何编程。但我认为具有对可能性更为普遍的认知，并知道如何跟程序员对话，是非常有帮助的。

如果你开始了，先学着走别急着跑。你需要说服你的同事和编辑，使用数据可以让你们得到其他方法得不到且值得去做的报道。一旦他们看到了这种方法的价值，你就可以向更复杂的报道和项目进军了。

我的建议是先学习Excel然后用它做一些简单的报道。从小处着手逐渐到数据库分析及数据制图。你可以在Excel中做很多事情——它是一个及其强大的工具，但大多数人对Excel功能的使用却是那么可怜兮兮。如果可以的话，参加一个为记者开设的Excel课程，比如新闻调查中心提供的课程。

带着敬畏之心去解读数据，不要轻视它。你必须要认真，要注重细节并且质疑你得出的结果。你需要保留处理数据的记录和原始数据的副本，因为在处理数据时候是很容易犯错误的。我经常要几乎从头到尾反复做两到三次分析来进行检查和验证。如果能让你的编辑或其他人分别分析数据并比较彼此的结果就更好了。
____

[quote, Scott Klein, ProPublica]
____
像记者撰写一个新闻报道那样一边快速写作，一边使用复杂的数据处理软件是一件相当了不起的事情。这在过去要花很长的时间。好在得益于在二十一世纪头十年的中期首次发布的Django和Ruby on Rails，这两个免费/开源的快速开发框架的出现，事情发生了变化。

Django是基于Python编程语言开发的，由阿德里安·霍洛瓦季和他位于堪萨斯州劳伦斯的劳伦斯日报世界版编辑部团队开发的。Ruby on Rails是由大卫·海涅迈尔·汉森和一个网络应用程序公司37Signals，在芝加哥开发的。

虽然这两个框架采取不同的方法来实现``MVC模式''，但它们都很出色，能快速地建立即使是非常复杂的网络应用程序。他们可以完成建立一个应用程序的基本的工作。比如创建并从数据库中获取项目、将URL与应用中特定的代码匹配。这些都被写进了程序的框架里，使开发人员并不需要编写代码来做这些基本的东西。

虽然一直没有对美国新闻app团队的正式调查，但通常大多数团队都使用这两个框架之一作为数据库支持的新闻应用。在ProPublica（一个非盈利调查机构）我们使用的就是Ruby on Rails。 

提供像亚马逊网络服务这样的快速网络服务器``切片''的发展，同样给过去开发一个应用缓慢的过程带来改观。 

此外，我们有很标准的工具去处理数据：用Google Refine和Microsoft Excel清理数据；用SPSS和R做统计; 用ArcGIS和QGIS去做GIS；用Git做源代码管理；用TextMate、VIM和Sublime Text写代码；用MySQL、PostgreSQL和SQL Server的组合做数据库。我们建立了我们自己的JavaScript框架，``Glass''，来帮助我们快速建立在JavaScript前端的大量应用。 
____

[quote, 谢丽尔·菲利普斯(Cheryl Phillips), 西雅图时报]
____
有时最好的工具就是最简单的工具——电子表格就是一种简便而又力量强大，却常常被我们被低估的工具。当所有东西都存储在磁盘操作系统下的时候，通过使用电子表格，我能够理解德州游骑兵股东们合伙协议中的复杂公式——而时逢乔治·W·布什恰是主要股东之一。电子表格可以帮助我标出异常值或计算错误。由此，我可以撰写出框架脉络或者更多的东西。

而这是数据记者“工具箱”里的基本装备。也就是说，我最喜爱的工具拥有更强大的功能——用SPSS做统计分析和地图程序，使我能看到地理上的模式。
____


[quote, 格雷格 艾许(Gregor Aisch), 开放知识基金会]
____
我是Python的超级粉丝。 Python是一种奇妙的开源编程语言，它很容易读写（例如，你不必在每行后键入一个分号）。更重要的是Python有一个庞大的用户群，因此对于你需要的一切都有插件（称为包）来实现。

我认为Django是数据记者很少会用到的东西。它是Python的一个网络应用框架，又称作创建大的、数据库驱动的网络应用工具。这对于小型交互式信息图表肯定有些“杀鸡焉用宰牛刀”了。

我也用QGIS，这是一个开源工具包，为需要不时处理地理数据的数据记者提供广泛的地理信息系统功能。如果您需要把地理空间数据从一种格式转换成另一种，那么QGIS就是你需要的。它可以处理几乎每一种地理数据格式（Shapefiles, KML, GeoJSON等）。如果你需要剪切出几个区域，QGIS也可以做到。并且，围绕着QGIS有一个庞大的讨论交流社区，所以你能够在网上找到众多像 http://bit.ly/goettingen-tutorial[教程]这样的自学资源。

R主要是作为一种科学可视化工具被创建的。很难找到一种还没有创建到R中的可视化方法或数据分析技术。R本身就是一个世界，是可视化数据分析的圣地“麦加城”。不够完美的一点是你需要（再一次）学习编程语言，因为R都有它自己的语言。但是，一旦你开始了在学习曲线上的攀爬，就没有什么工具比R更强大了。经过培训的数据记者可以用R来分析庞大的数据集，跨越Excel的限制（比如，你有一个一百万行的表）。 

R有一点非常不错，那就是对于处理数据的全过程，从读取CSV文件到生成表格，你都能够精确地记录下来。如果数据发生变化，可以一键再生成图表。如果有人怀疑图表的完整性，你可以向他展示确切的数据源，让每个人都可以自己生成这张图（或者找到你犯过的错误）。

NumPy + MatPlotLib几乎跟Python的功能是一样的。如果你已经很好地掌握了Python，NumPy + MatPlotLib只是你的一种选择。事实上，NumPy和MatPlotLib是Python程序包的两个例子。它们可以用于数据分析和数据可视化，但都局限于静态的可视化。它们不能被用于制作带有提示工具和高级素材的交互性图表

我不用MapBox，但我听说如果你想基于OpenStreetMap做较为复杂地图的话，它会是一个强大的工具。例如，它可以自定义地图风格（颜色、标签等等）。同时MapBox搭配一个叫Leaflet软件，基本上是用于绘制地图的一个更高级的JavaScript库，可以让你轻易地在地图供应商之间切换（OSM、MapBox、谷歌地图、必应……）。

RaphaelJS是一个相对低水平的可视化语言，允许你进行基本元素的处理（圆、线、文本），并把它们做成动画、进行交互等等。RaphaelJS里没有现成的图表，如柱状图，你得自己画。

但是，Raphael的优点是你做的一切都能在IE浏览器上正常运转。但其他很多的（令人赞叹的）可视化库，像d3，就都不支持IE了。悲剧的是很多用户仍用IE，但没有哪个编辑部能无视占据它们30%的用户的需求。

除了RaphaelJS，也其它可以给制作IE 版本Flash的工具替代品。《纽约时报》目前就在做这件事情。这意味着你得把每个应用开发两次。

我始终不认为存在为IE和主流的浏览器做可视化的所谓``最好''的工具。我经常发现Raphael在IE上跑得巨慢，几乎比在主流的浏览器中跑Flash慢上十倍。所以如果你想给所有的用户提供高质量的动画可视化，Flash替代版本也许是一个更好的选择。
____

[quote, Steve Doig, Walter Cronkite School of Journalism, Arizona State University]
____
我用的工具是Excel，它可以处理大部分CAR（计算机辅助报道）问题，并具有简单易学、大多数记者可快速掌握的优点。当需要合并表时，我通常使用Access，但会把合并后的表导出到Excel，做进一步的工作。我使用ESRI的ArcMap做地理分析，它很强大并且被收集地理编码数据的机构所使用。 TextWrangler在快速分析文本数据的布局及分隔方面很强大，并能用规则的表达式进行复杂的搜索和替换。当需要如线性回归这样的统计技术时，我用SPSS，它有一个友好的操作菜单。对于确实繁重的工作，比如处理数百万计的记录、需要认真筛选和程序化变量转换的数据集，我用SAS软件。
____

[quote, Brian Boyer, Chicago Tribune]
____
我们选择的工具包括Python和Django，用于破解、抓取和操控数据；PostGIS，QGIS和MapBox工具箱，用于建设复杂的网络地图。 我们正在考虑选择R语言还是NumPy+ MatPlotLib做探索性数据分析的工具，虽然目前我们最喜欢的数据工具是自主研发的CSVKit。我们所做的一切或多或少都是在云端部署的。
____

[quote, Angélica Peralta Ramos, La Nacion (Argentina)]
____
在《国家报》，我们使用：

*Excel去清洗、组织和分析数据；

*谷歌电子表格去发布、连接像谷歌Fusion Tables、Junar开放数据平台这样的服务；

*Junar用于分享我们的数据，并嵌入我们的文章和博客里；

*Tableau用于发布我们的交互式数据的可视化；

*Qlikview，一个非常快速的商业智能工具，我们用它来分析、筛选大型数据集；

*NitroPDF用来把PDF文件转换成文档和Excel文件；

*谷歌Fusion Tables用于地图可视化。

____

[quote, Pedro Markun, Transparência Hacker]
____
作为一个没有任何技术偏见的草根社区，我们“领军黑客”(Transparência Hacker)使用了很多不同的工具和编程语言。每一个成员都有他一套自己的喜好，这种巨大的差异性既是我们的长处也是我们的弱点。其实我们正在建设一个“透明黑客Linux发行版”，我们可以在任何地方live-boot，并随时进行数据破解。该工具包有一些有趣的工具，比如说Refine，RStudio和OpenOffice Calc（它是个被“聪明人”忽视的工具，但是在快速处理小型数据时确实非常有用）。此外，我们还使用了很多Scraperwiki快速制作原型和在线保存数据和结果。

对于数据可视化及作图，有很多我们喜欢使用的工具。Python和NumPy是很强大的。论坛里有人一直在用R语言，但归根结底我仍然认为Javascript绘图库，如d3,、Flot 和 Raphael，更为大部分项目所使用。最后，我们在绘制地图上进行了许多尝试，而Tilemill确实是一个有趣的工具。
____

=== Using Data Visualization to Find Insights in Data

[quote, William S. Cleveland (from Visualizing Data, Hobart Press)]
____
Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way. We discover unimagined effects, and we challenge imagined ones.
____

Data by itself, consisting of bits and bytes stored in a file on a computer hard drive, is invisible. In order to be able to see and make any sense of data, we need to visualize it. In this section I'm going to use a broader understanding of the term _visualizing_, that includes even pure textual representations of data. For instance, just loading a dataset into a spreadsheet software can be considered as data visualization. The invisible data suddenly turns into a visible "picture" on our screen. Thus, the question should not be whether journalists need to visualize data or not, but which kind of visualization may be the most useful in which situation.

In other words: when does it makes sense to go beyond the table visualization? The short answer is: _almost always_. Tables alone are definitely not sufficient to give us an overview of a dataset. And tables alone don't allow us to immediately identify patterns within the data. The most common example here are geographical patterns that can only be observed after visualizing data on a map. But there are also other kinds of patterns, which we will see later in this section.

==== Using Visualization to Discover Insights

It is unrealistic to expect that data visualization tools and techniques will unleash a barrage of ready-made stories from datasets. There are no rules, no "protocol" that will guarantee us a story. Instead, I think it makes more sense to look for "insights," which can be artfully woven into stories in the hands of a good journalist.

Every new visualization is likely to give us some insights into our data. Some of those insights might be already known (but perhaps not yet proven), while other insights might be completely new or even surprising to us. Some new insights might mean the beginning of a story, while others could just be the result of errors in the data, which are most likely to be found by visualizing the data.

In order to make finding insights in data more effective, I find the process discussed in <<FIG054>> (and the rest of this section) to be very helpful.

[[FIG054]]
.Data insights: a visualization (Gregor Aisch)
image::figs/incoming/05-BB.png[float="none"]

===== Learn how to visualize data

Visualization provides a unique perspective on the dataset. You can visualize data in lots of different ways. 

Tables are very powerful when you are dealing with a relatively small number of data points. They show labels and amounts in the most structured and organized fashion and reveal their full potential when combined with the ability to sort and filter the data. Additionally, Edward Tufte suggested including small chart pieces within table columns--for instance, one bar per row or a small line chart (since then also known as a sparkline). But still, as mentioned earlier, tables clearly have their limitations. They are great to show you one-dimensional outliers like the top 10, but they are poor when it comes to comparing multiple dimensions at the same time (for instance, population per country over time).

[[FIG055]]
.Tips from Tufte: sparklines (Gregor Aisch)
image::figs/incoming/05-BC-graphical-table.png[float="none"]

Charts, in general, allow you to map dimensions in your data to visual properties of geometric shapes. There's much written about the effectiveness of individual visual properties, and the short version is this: color is difficult, position is everything. In a scatterplot, for instance, two dimensions are mapped to the to the x- and y-position. You can even display a third dimension to the color or size of the displayed symbols. Line charts are especially suited for showing temporal evolutions, while bar charts are perfect for comparing categorical data. You can stack chart elements on top of each other. If you want to compare a small number of groups in your data, displaying multiple instances of the same chart is a very powerful way (also referred to as small multiples). In all charts you can use different kinds of scales to explore different aspects in your data (e.g., linear or log scale).

In fact, most of the data we're dealing with is somehow related to actual people. The power of maps is to reconnect the data to our very physical world. Imagine a dataset of geolocated crime incidents. Crucially, you want to see _where_ the crimes happen. Also maps can reveal geographic relations within the data (e.g., a trend from North to South, or from urban to rural areas).

[[FIG056]]
.Choropleth map (Gregor Aisch)
image::figs/incoming/05-BD-choropleth.png[float="none"]

Speaking of relations, the fourth most important type of visualization is a graph. Graphs are all about showing the interconnections (edges) in your data points (nodes). The position of the nodes is then calculated by more or less complex graph layout algorithms which allow us to immediately see the structure within the network. The trick of graph visualization in general is to find a proper way to model the network itself. Not all datasets already include relations, and even if they do, it might not be the most interesting aspect to look at. Sometimes it's up to the journalist to define edges between nodes. A perfect example of this is the http://slate.me/senate-social[U.S. Senate Social Graph], whose edges connect senators that voted the same in more than 65% of the votes.

===== Analyze and interpret what you see

Once you have visualized your data, the next step is to learn something from the picture you created. You could ask yourself:

  * What can I see in this image? Is it what I expected?
  * Are there any interesting patterns?
  * What does this mean in the context of the data?

Sometimes you might end up with a visualization that, in spite of its beauty, might seem to tell you nothing of interest about your data. But there is almost always _something_ that you can learn from any visualization, however trivial.

===== Document your insights and steps

If you think of this process as a journey through the dataset, the documentation is your travel diary. It will tell you where you have traveled to, what you have seen there, and how you made your decisions for your next steps. You can even start your documentation before taking your first look at the data.

In most cases when we start to work with a previously unseen dataset, we are already full of expectations and assumptions about the data. Usually there is a reason why we are interested in that dataset that we are looking at. It's a good idea to start the documentation by writing down these initial thoughts. This helps us to identify our bias and reduces the risk of misinterpretation of the data by just finding what we originally wanted to find.

I really think that the documentation is the most important step of the process--and it is also the one we're most likely to tend to skip. As you will see in the example below, the described process involves a lot of plotting and data wrangling. Looking at a set of 15 charts you created might be very confusing, especially after some time has passed. In fact, those charts are only valuable (to you or any other person you want to communicate your findings) if presented in the context in which they have been created. Hence you should take the time to make some notes on things like:

  * Why have I created this chart?
  * What have I done to the data to create it?
  * What does this chart tell me?

===== Transform data

Naturally, with the insights that you have gathered from the last visualization, you might have an idea of what you want to see next. You might have found some interesting pattern in the dataset which you now want to inspect in more detail.

Possible transformations are:

Zooming::
  To have look at a certain detail in the visualization
Aggregation::
  To combine many data points into a single group
Filtering::
  To (temporarily) remove data points that are not in our major focus
Outlier removal::
  To get rid of single points that are not representative for 99% of the dataset.

Let's consider that you have visualized a graph, and what came out of this was nothing but a mess of nodes connected through hundreds of edges (a very common result when visualizing so-called 'densely connected networks'). One common transformation step would be to filter some of the edges. If, for instance, the edges represent money flows from donor countries to recipient countries, we could remove all flows below a certain amount.

==== Which Tools to Use

The question of tools is not an easy one. Every data visualization tool available is good at something. Visualization and data wrangling should be easy and cheap. If changing parameters of the visualizations takes you hours, you won't experiment that much. That doesn't necessarily mean that you don't need to learn how to use the tool. But once you learned it, it should be really efficient.

It often makes a lot of sense to choose a tool that covers both the data wrangling and the data visualization issues. Separating the tasks in different tools means that you have to import and export your data very often. Here's a short list of some data visualization and wrangling tools:

  * Spreadsheets like LibreOffice, Excel or Google Docs
  * Statistical programming frameworks like R (r-project.org) or Pandas (pandas.pydata.org)
  * Geographic Information Systems (GIS) like Quantum GIS, ArcGIS, or GRASS
  * Visualization Libraries like d3.js (mbostock.github.com/d3), Prefuse (prefuse.org), or Flare (flare.prefuse.org)
  * Data wrangling tools like Google Refine or Datawrangler
  * Non-programming visualization software like ManyEyes or Tableau Public (tableausoftware.com/products/public)

The sample visualizations in the next section were created using R, which is kind of a Swiss Army knife of (scientific) data visualization.

==== An Example: Making Sense of US Election Contribution Data

Let us have look at the US Presidential Campaign Finance database, which contains about 450,000 contributions to US presidential candidates. The CSV file is 60 megabytes and way too big to handle easily in a program like Excel.

In the first step I will explicitly write down my initial assumptions on the FEC contributions dataset:

  * Obama gets the most contributions (since he is the president and has the greatest popularity).
  * The number of donations increases as the time moves closer to election date.
  * Obama gets more small donations than Republican candidates.

To answer the first question, we need to _transform_ the data. Instead of each single contribution, we need to sum the total amounts contributed to each candidate. After _visualizing_ the results in a sorted table, we can confirm our assumption that Obama would raise the most money:

[options="header"]
|=======================
|Candidate | Amount ($)
|Obama, Barack | 72,453,620.39
|Romney, Mitt | 50,372,334.87
|Perry, Rick | 18,529,490.47
|Paul, Ron | 11,844,361.96
|Cain, Herman | 7,010,445.99
|Gingrich, Newt | 6,311,193.03
|Pawlenty, Timothy | 4,202,769.03
|Huntsman, Jon | 2,955,726.98
|Bachmann, Michelle | 2,607,916.06
|Santorum, Rick | 1,413,552.45
|Johnson, Gary Earl | 413,276.89
|Roemer, Charles E. 'Buddy' III | 291,218.80
|McCotter, Thaddeus G | 37,030.00
|=======================

Even though this table shows  the minimum and maximum amounts and the order, it does not tell very much about the underlying patterns in candidate ranking. <<FIG059>> is another view on the data, a chart type that is called a "dot chart," in which we can see everything that is shown in the table _plus_ the patterns within the field. For instance, the dot chart allows us to immediately compare the distance between Obama and Romney, and Romney and Perry, without needing to subtract values. (Note: the dot chart was created using R. You can find links to the source code at the end of this chapter).

[[FIG059]]
.Visualizations to spot underlying patterns (Gregor Aisch)
image::figs/incoming/05-CC.png[float="0"]

Now, let us proceed with a bigger picture of the dataset. As a first step, I _visualized_ all contributed amounts over time in a simple plot. We can see that almost all donations are very, very small compared to three really big outliers. Further investigation reveals that these huge contributions are coming from the ``Obama Victory Fund 2012'' (also known as Super PAC) and were made on June 29th ($450k), September 29th ($1.5mio), and December 30th ($1.9mio).

[[FIG0510]]
.Three clear outliers (Gregor Aisch)
image::figs/incoming/05-DD.png[float="none"]

While the contributions by Super PACs alone is undoubtedly the biggest story in the data, it might be also interesting to look beyond it. The point now is that these big contributions disturb our view on the smaller contributions coming from individuals, so we're going to remove them from the data. This transform is commonly known as outlier removal. After visualizing again, we can see that most of the donations are within the range of $10k and -$5k.

[[FIG0511]]
.Removing the outliers (Gregor Aisch)
image::figs/incoming/05-EE.png[float="none"]

According to the contribution limits placed by the FECA, individuals are not allowed to donate more than $2500 to each candidate. As we see in the plot, there are numerous donations made above that limit. In particular, two big contributions in May attract our attention. It seems that they are 'mirrored' in negative amounts (refunds) in June and July. Further investigation in the data reveals the following transactions:

  * On May 10, _Stephen James Davis_, San Francisco, employed at Banneker Partners (attorney), has donated *$25,800* to Obama.
  * On May 25, _Cynthia Murphy_, Little Rock, employed at the Murphy Group (public relations), has donated *$33,300* to Obama.
  * On June 15, the amount of *$30,800* was refunded to _Cynthia Murphy_, which reduced the donated amount to *$2500*.
  * On July 8, the amount *$25,800* was refunded to _Stephen James Davis_, which reduced the donated amount to $0.

What's interesting about these numbers? The $30,800 refunded to Cynthia Murphy equals the maximum amount individuals may give to national party committees per year. Maybe she just wanted to combine both donations in one transaction, which was rejected. The $25,800 refunded to Stephen James Davis possibly equals the $30,800 minus $5000 (the contribution limit to any other political committee).

Another interesting finding in the last plot is a horizontal line pattern for contributions to Republican candidates at $5000 and -$2500. To see them in more detail, I visualized just the Republican donations. The resulting graphic is one great example of patterns in data that would be invisible without data visualization.

[[FIG0512]]
.Removing outliers 2 (Gregor Aisch)
image::figs/incoming/05-FF.png[float="none"]

What we can see is that there are many $5000 donations to Republican candidates. In fact, a look up in the data returns that these are 1243 donations, which is only 0.3% of the total number of donations, but since those donations are evenly spread across time, the line appears. The interesting thing about the line is that donations by individuals were limited to $2500. Consequently, every dollar above that limit was refunded to the donors, which results in the second line pattern at -$2500. In contrast, the contributions to Barack Obama don't show a similar pattern.

[[FIG0513]]
.Removing outliers 3 (Gregor Aisch)
image::figs/incoming/05-GG.png[scale="86",float="none"]

So, it might be interesting to find out why thousands of Republican donors did not notice the donation limit for individuals. To further analyze this topic, we can have a look at the total number of $5k donations per candidate.

[[FIG0514]]
.Donations per candidate (Gregor Aisch)
image::figs/incoming/05-HH.png[scale="86",float="none"]

Of course, this is a rather distorted view since it does not consider the total amounts of donations received by each candidate. The next plot shows the percentage of $5k donations per candidate.

[[FIG0515]]
.Where does the senator's money come from?: donations per candidate (Gregor Aisch)
image::figs/incoming/05-II.png[scale="88",float="none"]

==== What To Learn From This

Often, such a visual analysis of a new dataset feels like an exciting journey to an unknown country. You start as a foreigner with just the data and your assumptions, but with every step you make, with every chart you render, you get new insights about the topic. Based on those insights, you make decisions for your next steps and what issues are worth further investigation. As you might have seen in this chapter, this process of visualizing, analyzing and transformation of data could be repeated nearly infinitely.

==== Get the Source Code

All of the charts shown in this chapter were created using the wonderful and powerful software R. Created mainly as a scientific visualization tool, it is hard to find any visualization or data wrangling technique that is not already built into R. For those who are interested in how to visualize and wrangle data using R, here's the source code of the charts generated in this chapter:

  * https://gist.github.com/1769733[dotchart: contributions per candidate]
  * https://gist.github.com/1816161[plot: all contributions over time]
  * https://gist.github.com/1816169[plot: contributions by authorized committees]

There is also a wide range of books and tutorials available.

&mdash; _Gregor Aisch, Open Knowledge Foundation_