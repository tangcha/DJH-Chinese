:chapnum: 04
:figure-number: 00

[[getting_data]]
== 获取数据 ==

image::figs/incoming/04-00-cover.png[float="none",role="informal"]

++++
<?dbfo-need height="1in"?>
++++

那么下面，你已经做好了开始你第一个数据新闻项目的所有准备。那么紧接着该如何呢？首先，最重要的是你需要得到一些数据。这个部分着眼于你能够获取数据的来源有哪些。我们将会学习到如何在网上找到数据，如何运用信息自由法律向有关方面请求数据，如何使用“截屏”从非结构化的来源搜集数据以及如何使用“众包”从你的读者收集你自己的数据集。我们来看看哪些法律有关重新发布数据集以及如何使用简单的法律工具能够让别人也能够再使用你的数据。最后这部分以一些有关我们的贡献者是如何突破重围拿到他们想要数据的奇闻以及战争故事。

=== 5分钟的学科指南

寻找关于一个特定的主题或问题的数据？不确定有些什么数据或在哪儿找到这些数据？不知道如何入手？在这一小节我们来看看如何开始在网上寻找公共数据资源。

==== 精细你的搜索关键词

尽管这些数据很难总被轻易的找到，但是实际上很多数据库都已经被搜索引擎所收录，不论这是否出于发布者的本意。这里有几点建议：

  * 当你搜索数据时，一定要包括与你所要寻找的数据内容相关的搜索术语，以及你所希望的格式或来源的信息。谷歌和其他搜索引擎都允许你通过文件类型搜索。例如，你可以只搜寻电子表格(通过添加您搜索的文件类型："filetype:XLS filetype:CSV")，地理数据("filetype:shp")，或数据库提取("filetype:MDB，filetype:SQL, filetype:DB")。如果你愿意的话，你甚至可以查找PDF格式的（‘文件类型：pdf’）
  * 您也可以通过搜索URL的一部分。Google的"inurl:downloads filetype:xls"功能能帮你找到所有的已经``下载''到在他们网络服务器上Excel文件（如果你已经找到了单个下载文件，这个搜索关键词往往能帮你在服务器上同一文件夹中的找到其他结果）。你还可以限制只在某一个单一的域名中的结果，比如通过搜索"site:agency.gov"。
  * 另一个经常使用的诀窍是，不直接搜索内容，而是搜索可提供批量数据的位置。例如，"site:agency.gov Directory Listing"可以给你一些由服务器生成的容易获得的源文件的列表，如果用"site:agency.gov Database Download"的话就会为你搜寻人工创建的列表。

++++
<?dbfo-need height="2in"?>
++++

.直接寻找源文件
****
要说如何拿到公共数据，我的第一个绝招就是尝试直奔数据持有者，不是公众人物，也不是通过信息自由申请。我当然能精心制作一份通过信息自由法案的申请或者公开记录的请求，但是这会使得整个项目的运转变得很慢。很可能我就会得到回应说数据跟我申请的格式不一样，或者（像有些案例里那样）政府部门会使用专有软件就不能按我请求的数据格式那样提取出来。但是，如果我一上来就成功联系上持有某组织数据的人的话，我就可以直接问他关于关于某主题都有什么数据，他们是如何储存的。我还能够了解到数据的格式究竟是什么样的。再者，我可以用数据语言来精确描述我想请求的数据，知道如何做才能成功的请求到数据。你要问这种途径的障碍？时有发生，你很难联系上这些人。公开信息官员(PIO) 会想让我去跟他们谈。我发现在有的案例中，最好的方式是发起一个会议请求，当然如果能和公开信息官员，数据高手一起私下会面就再好不过了。我也能用一种让他们很难拒绝的方式来请求。``我不想给他们增添工作，''我说。``我不想给他们增添不必要的负担或者太广的请求，所以一次会面能让我确切的了解他们有什么，以及对我能最顺利准确请求到数据所必须知道的那些。''

如果这种方法不奏效，我的备案是在请求中首先就问他们数据记录的结构和数据字典。下一步我才真正的去申请数据本身。我有时也会问他们如何存放数据，存在什么系统里。通过这种方式我就可以研究这些数据都能用哪些方法导出，这对申请来说非常有好处。

最后要说的是，我最成功的一次经历来在当我还为蒙特纳的一家小报工作的时候。我需要一些统计数据，但是我被告知我想要的数据没法从主机中导出来。我当时研究了一番，然后主动请缨去帮助他们一起解决数据导出的问题。我和管数据的人一同，写了一些简短的代码，最终把数据打印到了软盘里（那是很久以前的事了）。我得到了我的数据，并且我们开发的这个统计小工具也被他们所配备，就能给请求数据的人提供数据了。他们没料想到这样的事会发生，但是有时他们自己也需要提取一些数据。他们完全不了解他们的系统，所以我们互相帮助。

&mdash; _谢丽尔·菲利普斯(Cheryl Philips)，西雅图时报_
****

==== 浏览数据网站和服务

近几年在网络上涌现出大量专门的数据门户网站、数据中心和其他数据网站，在这里你可以找到各种公开的数据。如果你是个新手，可以先去看看这些资源：

[[FIG042]]
.datacatalogs.org（开放知识基金会）
image::figs/incoming/04-01.png[float="0"]

官方数据门户::
  政府发布数据的意愿在国家之间差别很大。越来越多的国家都开设了数据门户网站（受美国的data.gov以及英国的data.gov.uk所启发）去促进民众或企业对政府数据的再利用。在 http://datacatalogs.org/[datacatalogs.org]这个网站上，你可以找到这些数据门户网站最新的索引信息。。另一个有用的网站是 http://www.guardian.co.uk/world-government-data[卫报世界政府数据]，这是一个元数据搜索引擎，囊括了许多国家的政府数据条目。

http://thedatahub.org/[The Data Hub]::
  一个由开放知识基金会运作的以社区推动型(community-driven)数据资源，这让寻找、分享、再利用这些开放数据变的非常简单，特别是以机器自动化的方式来进行数据操作。

https://scraperwiki.com/[ScraperWiki]::
  一个在线工具，其目的是“简化有用数据的提取，使这些数据便于应用到其他应用程序，或者提供给记者和研究人员”。大多数的数据提取网站及其数据库都是公开的，可以重复使用。

http://data.worldbank.org/[世界银行] 和 http://data.un.org/[联合国] 数据门户网站::
  世界银行和联合国的数据门户网站：为所有国家提供高水平的指标参数，数据通常可以追溯到多年以前。

http://buzzdata.com/[Buzzdata], http://www.infochimps.com/[Infochimps], 和 http://datamarket.com/[DataMarket]::
  一些旨在建立社区数据分享和转售的新兴公司。

http://datacouch.com/[DataCouch]::
  一个能上传、完善、分享及数据可视化的网站。

http://www.freebase.com/[Freebase]::
  Freebase是谷歌旗下的一个很有意思的子公司，“由一帮热爱开放数据的团体开发，提供人、地址以及物体的实体图”。

研究数据::
  许多国家和学科都会对科研数据进行汇总，如 http://www.data-archive.ac.uk/[英国数据档案]。其中有大量的数据可以免费访问，但也有不少是需要订阅，或需要管理机构同意才可使用和分发。

.从纸质文档中获取数据
****
正是在维基解密发布美国军方在阿富汗和伊拉克战争文档之后，我们决定遵循着这种概念，发布阿尔及利亚战争日记以纪念阿尔及利亚战争五十周年。我们开始去搜集并数字化法军在阿尔及利亚战争中的档案。这些可以在巴黎战争档案部都可以拿到，尽管都是纸质的。我们把这些档案分发给记者和学生，让他们把这些纸质档案拍成照片。我们也曾尝试过用佳能P-150便携扫描仪把他们扫描下来，但效果不是很好，主要因为这些档案都被装订过了。

最后，在几周之内收集到了大概有一万页的档案。我们试过用文字识别软件(ABBYY FineReader)去识别这些图片，但是结果不尽人意。还有就是，战争档案部门断然拒绝向我们提供另外几箱最有价值的档案。最重要的是，战争档案部禁止任何人再出版那些能被随意拍照关于地点的文档，所以我们决定不去冒这个风险，这个项目也就被搁置了。

&mdash; _尼古拉斯·凯瑟－布瑞尔（Nicolas Kayser-Bril），Journalism++_
****

==== 在论坛上发问

在 http://getthedata.org/[Get The Data]或 http://www.quora.com/[Quora]上搜索现成的答案或者提出问题。GetTheData是一个问答网站，你可以在上面问数据相关的问题，包括在哪里可以找到有关某一具体问题的数据、如何查询或检索某个特定的数据源、使用什么工具对数据进行可视化探索、如何净化数据或是如何转变成你可以使用的格式。

==== 在邮件列表中发问

邮件列表是整个团体在某个特定主题上的的智慧结晶。对于数据记者而言， http:/bit.ly/ddj-list[数据驱动新闻列表](Data Driven Journalism List)和 http://bit.ly/nicar-subscribe/[NICAR-L ]列表都是非常好的例子，不妨从它们开始。这些邮件列表上长期驻扎着从事各种项目的数据记者和计算机辅助报告 （CAR, Computer Assisted Reporting） 的极客。很可能其中有人做过跟你类似的项目，他即使不知道数据本身的链接，也可能有从何入手的想法。你也可以试试 http://project-wombat.org[Wombat项目]，``一个针对引用困难问题的讨论列表'', http://lists.okfn.org/mailman/listinfo[开放知识基金会的许多邮件列表]、http://theinfo.org/[theInfo]上的邮件列表，或寻找关于你所感兴趣的主题或领域的邮件列表。

==== 加入黑客/骇客(Hacks/Hackers)

http://hackshackers.com/[Hacks/Hackers]是一个在迅速扩张的国际草根新闻组织，在四大洲有着数十个分会和成千上万的成员。其任务是建立一个重新思考新闻和信息的未来的记者("hacks")和技术专家("hackers")的网络。在这样一个广泛的网络中，很有可能有人知道去哪里搜索你所要的数据。

==== 请教专家

教授、公务员和业界人士通常知道到哪里查找数据。给他们打电话、发电子邮件、找机会跟他们搭讪、拜访他们的办公室。然后彬彬有礼的询问：``我正在做一个关于 X 的报道。我在哪儿能找到相关数据吗？你知道谁有这方面的信息吗？''

==== 了解政府信息技术

了解各国政府在维护信息中所使用的技术和管理体系，这在访问数据时常常会很有帮助。不论是CORDIS（欧盟研究与发展计划相关资料库）、COINS或THOMAS，一旦你了解到一些关于这些缩略词所代表的大型数据库的预期目标，它们经常会成为你最有用的资料来源。

查找政府组织结构图，找出具有交叉职能（如报告、IT服务部门）的部门或单位，然后浏览他们的网站。很多数据保存在多个部门，可能在一个部门视作掌上明珠的某个数据库，在另一个部门就是免费的午餐。

在政府网站上寻找动态信息图表。这些图表通常是由可独立使用的结构化的数据源或应用程序编程接口所支持的（例如，飞行跟踪程序和天气预报的java应用程序）。

.用电话记录来“钓鱼”
****
几个月之前，我想去剖析时任总统候选人所在的得克萨斯州政府。具体来说，我想要瑞克佩里(Rick Perry)的手机通话记录。那是我们已经期待已久的国家公开记录的申请结果。拿到手的数据是120页以上只有传真质量的档案。我们颇费了一番功夫进行数据录入和清理，再通过WhitePages.com的API去反向查询电话号码

++++
<?dbfo-need height="1in"?>
++++

将这些人名与州和联邦政府(FEC)选举数据对应起来以后，我们发现佩里通过 http://bo.st/perry-phone[州政府工作电话]伸手拿到了大量的选举经费和超级政治行动委员会(PAC, Political Action Committee)资助，这种令人不悦的做法掀起了对他和他所倾心的``超级政治行动委员会''之间勾连的质疑。

&mdash; _杰克·吉勒姆(Jack Gillum)，美联社_
****

==== 重复尝试搜索

当你知道更多数据相关的信息后，用你上次搜索所注意到的重要关键词组再搜索一次。这样你没准就走运搜到了你想要的数据！

==== 撰写一个信息自由请求

如果你认为政府机构握有你所需要的数据，写一个信息自由（Freedom of Information）请求书可能是最好的办法。有关如何撰写文件更多信息请参阅下一章节。

&mdash; _布赖恩·博耶（芝加哥论坛报），约翰·基夫（美国纽约公共广播公司），弗里德瑞克·林登伯格（开放知识基金会），简·帕克（Creative Commons）, 克里斯·吴（Hacks/Hackers）_

.当法律失效
****
After reading a http://bit.ly/hygiene-inspections[scholarly article] explaining that publishing the outcome of hygiene inspections in restaurants reduced the number of food-related illnesses in Los Angeles, I asked the Parisian hygiene services for the list of inspections. Following the procedure set out by the French FOIA, I waited 30 days for their refusal to answer, then went to the Access to Public Data Commission (CADA in French), which rules on the legitimacy of FOI requests. CADA upheld my demand and ordered the administration to release the data. The administration subsequently asked for two months extra time, and CADA accepted that. Two months later, the administration still hadn't done anything.

I tried to get some big-name (and big-pocketed) open data advocates to go to court (which is a €5000 affair and a sure win with CADA support), but they were afraid to compromise their relations with official open data programs. This example is one among several where the French administration simply ignores the law and the official initiatives do nothing to support grassroots demands for data.

&mdash; _Nicolas Kayser-Bril, Journalism++_
****

++++
<?dbfo-need height="2in"?>
++++

=== 你对数据的权利

在发出信息自由 (FOI) 申请之前，你应该查一下正在搜索的数据是否已经公开，或者是否已经有人提出过申请。你可以从上一章查阅其中的几个建议。如果你已经找了一圈还是没有得到所需要的数据，你可能就想要提交一份正式的申请。这里有一些提示，可以让你的申请更为有效。

提前计划 节省时间::
  每当你在搜索信息的时候，就要考虑提交一份正式的访问请求。最好不要等到用尽其他办法再作打算。在研究开始之时提交请求，同时开展其他调研，这样会为你节省时间。对拖延有所准备：公共机构有时需要一段时间来处理请求，所以你最好是对这一情况有所预期。

查看收费规定::
  在开始提交申请之前，查一下有关提交申请或接收信息的收费规定。这样一来，如果政府官员突然问你要钱，你会对自己的权利心里有数。你可以索要电子文档来避免拷贝和粘贴的成本，所以在申请中要写清楚你更希望获得电子格式的信息。这样就可以避免支付费用，除非是信息没有电子文档。不过现今通常可以将没有数字化的文件扫描，而后以电子邮件的附件的形式发送。

知晓你的权利::
  在开始之前搞清楚自己的权利，这样你就知道自己拥有何种权利以及公共部门的义务所在。例如，大部分信息自由法对当局的回复有一个时间限制。在全世界的大多数法律中，该范围从几天到一个月不等。在你申请之前确定这一期限，并在提交申请时作好记录。

政府没有为你处理数据的义务，但应当向你提供他们所有的资料。如果根据政府所履行的法律能力应当提供某个数据，那么他们肯定应当为你制作。

声明你知道你的权利::
  通常法律并没有要求你提及访问信息法或者信息自由法案，但建议你这样做，因为它表明你知道自己的合法权利，并且可能鼓励依法正确处理申请。我们注意到对于欧盟的申请，其中重要的一点是写明这是一个文件访问申请，而且最好具体写明是提案1049 /2001。

保持简洁::
  不论在哪个国家，最好都从一个简单的信息申请开始，如果得到了初步信息，然后再增加更多的问题。这样，你就不会因为提出一个``复杂的请求''而冒被公共机构申请延期的风险。

保证重点::
  申请由公共部门的一部分保有的信息，可能会比需要搜索整个部门的回复来得更快。需要官方咨询第三方（例如提供信息的私营公司、受其影响的其他政府）的申请可能会花费特别长的时间。你要持之以恒。

考虑文件所包含的信息::
  试着找出所整理的数据。举个例子，假如你在交通事故后拿到一张警察填写的表单空白副本，你就可以看出他们记录了哪些有关车祸的信息。

针对具体问题::
  在你提交申请之前，想想：它有什么含糊不清的地方吗？如果你计划比较来自不同官方部门的数据，这一点尤其重要。打个比方，如果你索要过去三年的数字，一些部门会发给你过去三个日历年的信息，而其它部门则发给你过去三个财政年的信息，你不可能直接比较这些信息。如果你决定要把你真正的申请隐藏在一个更普遍的申请当中，那么你的申请范围应当足以获取你想要的信息，但也能太过泛泛而用意不明或有碍回复。具体而明确的申请往往能够获得更快更好的答案。

提交多个申请::
  如果你不确定向谁提交申请，你完全可以在同一时间向两三个或更多的机构提交申请。在某些情况下，各机构会返回不同的答案，但这实际上是有帮助的，可以为你所调查的项目上提供更全面的信息。

提交国际申请::
  越来越多的申请可以以电子方式提交，所以你住在哪里并不重要。或者，如果你没有生活想要提交申请的国家中，有时可以将申请发送到大使馆，而他们应将其转移给公共机构。你需要先查看相关使馆是否有这类服务，有时使馆工作人员没有接受过对信息权利的培训，如果看似是这种情况，直接向有关公共机构提交申请是更安全的做法。

进行申请测试::
  如果在你打算给许多公共当局发送同一申请，开始的时候可以给几个部门发一份初步的申请草案作为提前测试。这会告诉你是否使用了正确的术语来获取想要的材料，以及回复你问题的可行性，这样你就可以在发送给各个部门之前对申请进行必要的修改。

考虑好意外情况::
  如果觉得你的申请可能会出现意外，那么在准备申请之时，你可以把可能存在敏感信息的问题与其它根据常识不会出现意外的信息分开。然后把你的问题分成两项申请并分别提交。

请求对文件的访问::
  如果你住在保存信息的机构附近（例如在存放文件的首都），你也可以要求查看文件的正本。当研究信息可能保存在大量的你想查阅的文献中时，这会是非常有用的。这类查阅应当是免收费用的，而且可以给你安排在一个合理和方便的时间。

自己留一份记录！:: 
  以书面形式作出申请，并保存一份副本或记录，以便在将来如果未收到答复需要作出上诉时，能够证明你的申请已经发送。并且假如你打算做一个有关报道，这也可以提供了申请提交的证据。

公开你的申请::
  通过把你提交的申请公开化可以加速回复的速度：如果你撰写或广播一个关于你已提交申请的报道，这可以对公共机构施加压力使其处理和回复你的申请。在收到对申请的回复后，你可以更新信息；如果超过截止时间仍没有回复，你也可以把这做为一个新闻报道。这样做还有一个好处，就是教育大众有关信息的访问权以及如何实践。


[提示]
====
在网上有几个公开的优质的服务机构，你可以用来提交申请以及任何后续的回复，如英国公共机构的 http://www.whatdotheyknow.com/[What Do They Know?]、德国公共机构的 https://fragdenstaat.de/[Frag den Staat]和欧盟机构的 http://www.asktheeu.org/[Ask the EU]。而 http://www.alaveteli.org/[Alaveteli]项目正在帮助世界各地的数十个国家开展类似的服务。
====

[[FIG043]]
.What Do They Know? (My Society)
image::figs/incoming/04-AA.png[float="none"]

发展你的同事::
  如果你的同事对信息申请访问的意义持怀疑态度，说服他们的最佳途径之一就是根据信息法所访问到信息写一篇报道。在最后一篇文章或广播片段中也把你所用到的法律推荐给公众，作为一种强调其价值并提高公众的权利意识的方法。

索要原始数据::
  如果你想要用电脑分析、挖掘或整理数据，那么你应当明确索要电子化的机器可读的数据格式。你可以通过详细说明来阐明你的要求，例如你需要``适用于会计软件分析''格式的预算信息。你可能还希望明确索要非汇总过的，或特定粒度的信息。关于这一点，你可以扩展阅读 http://bit.ly/access-report[此报告]。

++++
<?dbfo-need height="1in"?>
++++

询问FOI法律之外的组织::
  你可能希望找到有关非政府组织、私营公司、宗教组织和其他组织中在FOI法律下并不需要公开的文件。但是通过询问FOI法律所涵盖的公共机构你可能找到有关的信息。例如，你可以询问政府部门或部委它们是否资助过或处理过某个特定私人公司或非政府组织，并申请支持文件。如果在 进行FOI申请需要进一步帮助，你还可以查阅 http://www.legalleaks.info/toolkit.html[记者所应该知道的法律漏洞](Legal Leaks tookit for journalists)。

&mdash; _海伦·达比希尔（Access Info Europe）、Djordje·Padejski（斯坦福大学奈特新闻会员）、马丁·罗森鲍姆（英国广播公司）和法布里齐奥·斯科诺利尼（伦敦政治经济学院）_

.Using FOI to Understand Spending
[[foi-spending]]
****
I've used FOI in couple of different ways to help cover COINS, the UK Government's biggest database of spending, budget and financial information. At the beginning of 2010, there was talk from George Osborne that if he became chancellor, he would release the COINS database to facilitate greater transparency in the Treasury. At this time it seemed a good idea to investigate the data in and structure of COINS so I sent a few FOI requests, one for the http://bit.ly/wdtk-coins-1[schema of the database], one for the guidance Treasury workers receive when http://bit.ly/wdtk-coins-2[they work with COINS], and one for the Treasury http://bit.ly/wdtk-coins-3[contract with the database provider]. All of which resulted in publication of useful data. I also requested all the spending codes in the database, http://bit.ly/wdtk-coins-4[which was also published]. All of this helped to understand COINS when George Osborne became chancellor in May 2010 and published COINS in June 2010. The COINS data was used in a number of websites encouraging the public to investigate the data--including http://openspending.org/[OpenSpending.org] and the Guardian's http://coins.guardian.co.uk/coins-explorer/search[Coins Data Explorer].

After further investigation it seemed that a large part of the database was missing: the Whole of Government Accounts (WGA) which is 1,500 sets of accounts for public funded bodies. I used FOI to http://bit.ly/wdtk-coins-5[request the 2008/09 WGA data] but to no avail. I also asked for the report from the audit office for WGA--which I hoped would explain the reasons the WGA was not in a suitable state to be released. That was http://bit.ly/wdtk-coins-6[also refused].

In December 2011, the WGA was released in the COINS data. However I wanted to make sure there was enough guidance to create the complete set of accounts for each of the 1,500 bodies included in the WGA exercise. This brings me on to the second way I used FOI: to ensure the data released under the UK transparency agenda is well-explained and contains what it should. I put in a FOI request for the http://bit.ly/wdtk-coins-7[full set of accounts for every public body included in WGA].

&mdash; _Lisa Evans, the Guardian_
****

=== 试试游说(Wobbing)数据！

利用信息自由法案，有时也被称之为数据游说，是非常有效的工具。但它需要一定的方式方法，往往更要靠毅力。这里用三个我作为调查记者的亲身经历，来说明数据游说的长处与挑战。

==== 个案研究1：农业补贴

欧盟每年将近补贴600亿欧元给农民以及整个种植业。没错，是每年。从20世纪50年代后期开始持续到现在，这一直作为政策上对贫穷农民补贴。然而在2004年丹麦，作为FOI的第一次突破，揭露了这仅仅是政策上的表述而已，并没有落到实处。小农场主们像他们时常私下里或公开抱怨的那样不断挣扎着，事实上大部分的钱都流向了少数大地主手中或农产业里。所以，很显然我想知道：在欧洲都是这样的吗？

2004年的夏天，我向欧盟委员会索要数据。每年的二月，委员会都会收到各成员国的数据。数据显示谁申请欧盟拨款，受资助的受益人得到多少，以及他们是否通过耕作他们的土地、开发他们的地区或者出口奶粉而得到。当时，委员会以存在CD光盘的CSV格式文件收到数据。虽然数据量很大，但原则上这都很容易完成的工作。前提是你能得到它的话，那的确是。

在2004年，委员会拒绝公开数据；主要的争论点是，数据上传到数据库之后还要做大量的工作，才能把想要的数据提取出来。按欧洲司法监察机构的说法来看，这就是_行政失当_。现在你可以在 http://bit.ly/eu-wobbing[wobbing.edu的网站上]找到有关这个案例的全部文档。回到2004年，我们那时候可没有时间一步一步走法律程序。我们想要的是数据。

[[FIG044]]
.农业补贴网站 (Farmsubsidy.org)
image::figs/incoming/04-BB.png[float="0"]

所以我们和一些同伴组成团队，为了获取数据跑遍了欧洲的每一个国家。英国、 瑞典、和荷兰的同事拿到了2005 年的数据。芬兰、 波兰、 葡萄牙、 西班牙、 斯洛文尼亚和其他一些国家也开放了他们的数据。即使在最难的德国，我也获得了重大突破，拿到了威斯特伐中省的北莱茵-利亚(North Rhine-Westfalia)一些2007 年的数据。为了拿到数据我不得不走上法庭——最终的结果是，一些相当优秀的报道文章刊登在了 http://bit.ly/stern-wobbing[Stern and Stern在线新闻杂志]上。

难道丹麦和英国最早开放他们的数据是巧合吗？不一定。从更大的政治图景来看，彼时农业补贴问题正在世界贸易组织谈判中被施压。因为丹麦和英国属于欧洲里更偏向自由派的国家，所以这些透明的政治风向更可能吹向他们。

报道仍在继续，更多的事件和数据请查阅 http://farmsubsidy.org/[农业补贴官方网站]。

心得：去各个地方“游说”数据。在欧洲，我们有多种多样到令人吃惊的信息自由法，并且不同的国家在不同的时候会有不同的政治兴趣。这都可以成为你的优势。

.知晓你的权利
****
当你发布数据的时候，你不是应该考虑一下版权问题和其他有关数据的权利？虽然你应该合你的法律团队一起搞清所有的法律问题，但一般来说：如果数据是由政府发布的，那你既不用请求宽恕也不用请求许可；如果这是由组织发布的，且数据并不是为了盈利的话，那你也不用太过操心；如果这是组织发布的数据且是出于盈利目的的话，那你就一定要去请求允许。

&mdash; _西蒙·罗杰斯(Simon Rogers)，卫报_
****

==== 个案研究2：副作用

吃药的时候我们都是被拿来作试验的小白鼠。药物都会有副作用。尽管众所周知，我们会在衡量过潜在的好处和风险之后，再作出（是否服用的）决定，但不幸的是这一决定往往都不是明智的。

++++
<?dbfo-need height="1in"?>
++++

青少年们是为了拥有更光滑的皮肤服而服用抗粉刺药，而不是想让自己变的抑郁。这样的事恰恰就发生在一种抗粉刺药上，青少年服用之后变得抑郁，甚至导致自杀。这种危险的特殊副作用的报道对记者来说是显然是非常重要的，但是却很难容易操作。

有关药物副作用的数据是有的。生产者必须定期向卫生当局提供观察到的有关副作用的情况。从药物被允许进入市场开始，国家或者欧洲有关当局就已经拿到这些数据。

最初的进展再一次由丹麦的国家层面开始。在一个由丹麦-荷兰-比利时三国团队进行的跨境研究期间，荷兰也开放了他们的数据。数据游说的另一个例子： 我们的这个案例可以明确指出一点，荷兰当局的数据可以再丹麦拿到。

但这是个真实的故事：在欧洲的许多国家都发现，有自杀倾向的年轻人很悲剧地因为服食药物而最终自杀。新闻工作者、 研究人员和年轻受害者的家属都在用尽全力去获取这些信息。欧洲监察员也在帮助推动在欧洲药品管理局的透明公开，而且看起来 http://bit.ly/eu-ombudsman[好像成功]了。所以现在的任务落到了记者头上，摆出数据并彻底剖析这些材料。我们都是豚鼠吗？正如一位研究人员所说的那样说的，或者，控制机制健全嘛？

心得：关于信息透明公开的问题绝对不要妥协。坚持下去并且随着故事的发展推进下去。事情可能会有很好的转机，或许因此在短时间内就能拿到更好的数据，写出更好的报道。

==== 个案研究3：走私死亡

最近的历史发展对全人类来说都异常煎熬，尤其是在战后和转型时期。记者又如何可以获得“干货”数据进行调查, 例如最近十年的战争奸商何时开始掌权当下？这正是一个由斯洛文尼亚、 克罗地亚、 波斯尼亚记者所组成的团队所追求的目标。

该团队的目的是调查90年代初联合国禁运期间，前南斯拉夫境内的武器交易。工作的基础是议会对这个议题的调查记录。然而，为了记录下来他们的运送路线并了解交易结构，记者们还必须要去跟踪港口的船只数量和卡车的车牌。

斯洛文尼亚议会委员会曾主持调查从巴尔干战争谋取暴利的问题，但从来没有得出什么结论。然而他们尚有解密文件和数据中极富价值的线索，包括斯洛文尼亚团队通过信息自由请求拿到的6,000页的文件。

In this case the data had to be extracted from the documents and sorted in databases. By augmenting the data with further data, analysis, and research, they were able to map numerous routes of the http://bit.ly/kaasogmulvad-smuggling[illegal weapon trade].
在这种情况下,数据还必须从文件中提取出来并在数据库中分类整理好。通过补充更多的数据一同进行分析和研究，他们绘制出了大量的 http://bit.ly/kaasogmulvad-smuggling[非法武器贸易路线]。

整个团队非常的成功，结果也很 http://bit.ly/journalismfund-smuggling1[独特]，并且为团队赢得了他们的第一个奖项。最重要的是，这些报道影响了整个地区。同时，其他国家的记者还能够延续他们的工作，继续这些死亡货物运输路径的报道。

心得：可以尝试在最意想不到的地方挖掘好的原素材，再与一些已经公开的数据结合起来（进行分析）。

&mdash; _布里奇特·阿尔夫特(Brigitte Alfter)，Journalismfund.eu_

.FOI with Friends
****
Many Balkan countries have issues with government corruption. Corruption is often even higher when it comes to accountability of the local governments in those countries. For several months a group of Serbian journalists around the Belgrade-based http://www.cins.org.rs/[Centre for Investigative Reporting] have been questioning different types of FOI documents from over 30 local municipalities in 2009. Prior to that, almost nothing was accessible to the public. The idea was to get the original government records and to put the data in spreadsheets, to run basic checks and comparisons among the municipalities and to get maximum and minimum figures. Basic indicators were budget numbers, regular and special expenses, salaries of officials, travel expenses, numbers of employees, cell phone expenses, per diems, public procurement figures, and so on. It was the first time that reporters had asked for such
information.

The result was a comprehensive database that unravels numerous phony representations, malfeasances, and corruption cases. A list of the highest-paid mayors indicated that a few of them were receiving more money than the Serbian president. Many other officials were overpaid, with many receiving enormous travel repayments and per diems. Our hard-earned public procurement data helped to highlight an official mess. More than 150 stories came out of the database and many of them were picked up by the local and national media in Serbia.

We learned that comparing the records with the comparable data from similar government
entities can display deviations and shed light on probable corruption. Exaggerated and unusual expenses can be detected only by comparison.

&mdash; _Djordje Padejski, Knight Journalism Fellow, Stanford University_
****

=== Getting Data from the Web

You've tried everything else, and you haven't managed to get your hands on the data you want. You've found the data on the Web, but, alas--no download options are available and copy-paste has failed you. Fear not, there may still be a way to get the data out. For example you can:
你是否已尝试了各种方法，却仍未获得需要的数据？也许有时你在网页上已经找到所需数据了，只是上面并没有下载按钮，复制粘贴功能也用不了。不要着急，这里有一些实用的方法，比如你可以：

++++
<?dbfo-need height="1in"?>
++++

  * 从基于网页的API接口获得数据，这包括在线数据库提供的用户界面以及各种新式的网络应用（比如Twitter、Facebook等等）。这是获得政府和商业机构数据的好方法，在社交网站上也很有效。
  * 从PDF文档提取数据。这很困难，因为PDF是一种针对打印机的格式，里面存储的数据结构和一般文档极为不同。从PDF提取数据比从一本书中提取要困难得多，但还是有一些工具和操作指南可以帮助你完成这项工作。
  * 利用有网页抓取功能的网站。在这类网站上，你可以借助其提供的实用工具或是自己写一段建议代码从普通网页上提取结构化的内容。这种方法十分强大，适用于许多情况，但这要求你了解一些关于网页的知识。

借助这些强大科技功能的同时，也别忘了简单易用的方法：花点时间搜索机器可读的数据，或者给持有所需数据的机构打电话都可能会帮助你拿到你想要的数据。

在本节我们将展示一则从HTML网页上极为简单的抓取范例。

==== 什么是机器可读的数据？

大多数方法的目的都是为了获得机器可读的数据。机器可读的数据是为方便计算机处理而生成的，而不是为了向人类用户展示。这些数据的结构与其内容相关，但与数据的最终展示形式不同。简单的机器可读数据格式包括CSV、XML、JSON和Excel文档等等，而Word文档、HTML网页和PDF文档则更侧重于数据在视觉上的呈现。PDF是一种与打印机交互的语言，它记录的信息并不是一个个字母，而是线与点在页面上的位置。

==== 从网页上抓取什么？

这种事情每个人都做过：你在某网站上浏览时发现一个有趣的表格，想把它复制到Excel中便于计算或是存储下来。但有时这种方法并不奏效，有时你所需要的数据又分布在好几个网站的页面上。手动复制粘贴太乏味了，而用一些小代码可以令你事半功倍。

网页抓取的一大优势是其几乎可以用于所有网站，无论是天气预报还是政府预算。即便该网站并未提供针对原始数据访问的API接口，你同样可以抓取。

==== 网页抓取的局限性

抓取不是万能的，也会遇到障碍。网页难以抓取的主要因素有：

  * HTML编码拙劣，结构信息很少或者压根没有，常见于早期的政府网站。  
  * 网站有防止机器自动访问的验证系统，如CAPTCHA验证码和付费系统。
  * 使用浏览器Cookies存储用户信息获得用户动作再给出内容的会话系统。
  * 网站未提供完整的分类列表和通配符搜索功能。
  * 服务器管理员对大量访问做出了限制。

另一方面，法律限制也会成为障碍。部分国际承认关于数据库的权利，这会限制你重复利用在网络上公开发表的信息。有的时候，你可以无视这些法律条款仍然进行抓取，这取决你所在地的司法管辖权，如果你是记者的话也会有一些特殊的便利。抓取免费的政府数据一般没事，不过在发表之前还是应当再查一遍。商业组织和部分NGO对数据抓取行为采取几乎零容忍的态度，他们会指控你“破坏”他们的系统。其他可能侵犯个人隐私的数据则会触犯数据隐私法令，也与职业道德相背。

.Patching, Scraping, Compiling, Cleaning
****
The challenge with huge swathes of UK data isn't getting it released--it's getting it into a usable format. Lots of data on hospitality, MPs' outside interests, lobbying and more is routinely published but in difficult-to-analyze ways.

For some information, there is only the hard slog: patching together dozens of Excel files, each containing just a dozen or so records, was the only way to make comprehensive lists of ministerial meetings. But for other information, web scraping proved incredibly helpful.

Using a service like ScraperWiki to ask coders to produce a scraper for information like the Register of MPs' interests did around half of our job for us: we had all MPs' information in one sheet, ready for the (lengthy) task of analysing and cleaning.

Services like this (or tools such as Outwit Hub) are a huge help to journalists trying to compile messy data who are unable to code themselves.

&mdash; _James Ball, the Guardian_
****

==== 抓取工具

有许多程序可用于从网站提取大量信息，包括浏览器扩展程序和一些网络服务。 http://www.readability.com/[Readability]（从网页上抓取正文）和 http://www.downthemall.net/[DownThemAll]（批量下载文件）工具可以在部分浏览器上自动处理繁琐的任务，Chrome浏览器的 http://bit.ly/chrome-scraper[Scraper插件]可以从网站上提取表格。针对开发者的扩展程序FireBug（针对Firefox浏览器，Chrome、Safari和IE已内置类似功能）可以让你清晰了解网站结构和浏览器与服务器之间的通讯。

https://scraperwiki.com/[ScraperWiki]网站提供包括Python、Ruby、PHP在内的多种语言供用户自行编写抓取代码。这使得用户不再需要在本地安装语言环境便可编码进行抓取工作。另外Google电子表格和Yahoo! Pipes等网页服务也提供从其他网站提取内容的服务。

==== 网页抓取工具如何运作？

网络抓取工具通常是用Python、Ruby或PHP写成了一小段程序代码。具体选择哪一种语言取决于你的周围，如果你的新闻机构或者同城市的同行中有人已开始用某种语言进行编写，你最好也采用同样的语言。

虽然前文提到的点击选择工具可以帮助你上手，但真正复杂的步骤是确定正确的页面和页面上存储所需信息的正确元素。这些步骤的关键并不在于编程，而在于对网站和数据库结构的了解。

浏览器在展现网页时主要运用以下两种技术：通过HTTP协议与服务器通讯，请求获得文档、图片、视频等指定资源；然后获得以HTML编码写成的网页内容。

==== 网页的构造

每个HTML网页都是由有一定结构层次的“盒子”构造的（由HTML``标签''定义）。大的“盒子”中又会包含小的“盒子”，就像一个表格中有行、列和单元格一样。不同的标签有不同的功能，可以定义“盒子”、表格、图片或者是超级链接。标签也有附加属性（比如唯一标识符），并可被定义在“类”中，这便于我们定位和获取文档中的独立元素。编写抓取工具的核心就是选择合适的元素从而获取对应的内容。

查看网页元素时，所有代码都可按照“盒子”进行分割。
 
在开始抓取网页之前，你需要了解HTML文档中会出现哪些类型的元素。举例来说，+<table>+会形成一个表格，在其中+<tr>+定义了行，+<td>+又把行细分为单元格。最常见的元素类型是+<div>+，简单来说它可以定义任何内容区域。认知这些元素最简单的方法就是利用浏览器上的 http://bit.ly/developer-toolbar[开发者工具]，在将鼠标悬停在网页的特定区域上时，这些工具就会自动显示该区域对应的代码。

标签就像书的封面一样，告诉你哪里是开头，哪里是结尾。+<em>+_表示文字从此处开始以斜体显示_，+</em>+则标明斜体字到此结束。多简单！

==== 例子：使用Python抓取核事件

国际原子能机构(IAEA)门户网站上的 http://www-news.iaea.org/EventList.aspx[新闻栏目]下记录了全球各地的放射性事故（栏目名正申请加入“怪异标题俱乐部”）。该网页使用简单、类似博客形式的结构，便于抓取。

[[FIG045]]
.国际原子能机构(IAEA)门户网站 (news.iaea.org)
image::figs/incoming/04-CC.png[float="none"]

首先，在 https://scraperwiki.com/[ScraperWiki]上新建一个Python抓取工具，然后你将看到一个基本空白的文本框，里面有些基本的框架代码。同时在另一个窗口中打开 http://www-news.iaea.org/EventList.aspx[IAEA网站]，并打开浏览器的开发者工具。在“元素”视图下，找到每条新闻标题所对应的HTML元素，开发者工具会明确指出定义标题的代码。

进一步观察可以发现，标题用+<h4>+定义在+<table>+中。每个事件都有一个单独的+<tr>+行，里面还有事件描述和日期。为了获取所有事件的标题，我们应当用一定的方法按顺序选择表格中每一行，然后获得标题元素中的文本。

要将这些进程写成代码，我们需要明确具体的步骤。我们先玩个小游戏感受一下什么是步骤。在ScraperWiki的界面中，先尝试为自己写一些指引，你要通过代码完成哪些工作，就像食谱中的工序一样（每行开始前写一个“#”以告诉Python这行不是计算机代码）。例如：

----
# 寻找表格中的所有行
# 不要让独角兽在左侧溢出（注：IT冷笑话）
----

写的时候要尽可能准确，不要认为程序真的懂你要抓取哪些内容。

写了几行伪代码后，我们再来看看真正代码的前几行吧：

----
import scraperwiki
from lxml import html
----

在第一段中，我们从库（预先写好的代码片段）中调用了已经存在的功能，+ScraperWiki+在此段代码中已经提供了下载网站的功能，+lxml+则是一个用来对HTML文档进行结构分析的工具。告诉你个好消息，在ScraperWiki中写Python的抓取工具，前两行都是一样的。

----
url = "http://www-news.iaea.org/EventList.aspx"
doc_text = scraperwiki.scrape(url)
doc = html.fromstring(doc_text)
----

然后，代码定义了变量名称：+url+，其值为IAEA的网页地址。这行告诉抓取工具，有这么个事情，我们要对他做些动作。注意这段URL网址在引号中，表明这不是一段代码，而是一个_字符串_，一串字符序列。

然后我们将这段+URL+变量放入一个指令，+scraperwiki.scrape+。这段指令会执行已定义好的动作：下载网页。这段工作完成后，它将执行指令将内容输出到另一个变量+doc_text+中，然后在+doc_text+中存储的就是网页的文本了。不过这段文本并不是你在浏览器中看到的样子，它是以源代码形式存储的，包含了所有的标签。由于这些代码不容易解析，我们再用另一个指令+html.fromstring+生成一个特殊的格式，方便我们分析其中元素，这种格式叫做文档对象模型(DOM)。

----
for row in doc.cssselect("#tblEvents tr"):
link_in_header = row.cssselect("h4 a").pop()
event_title = link_in_header.text
print event_title
----

In this final step, we use the DOM to find each row in our table and extract the event's title from its header. Two new concepts are used: the for loop and element selection (+.cssselect+). The for loop essentially does what its name implies; it will traverse a list of items, assigning each a temporary alias (+row+ in this case) and then run any indented instructions for each item.

The other new concept, element selection, is making use of a special language to find elements in the document. CSS selectors are normally used to add layout information to HTML elements and can be used to precisely pick an element out of a page. In this case (line 6) we're selecting +#tblEvents tr+, which will match each +<tr>+ within the table element with the ID +tblEvents+ (the hash simply signifies ID). Note that this will return a list of +<tr>+ elements.

That can be seen on the next line (line 7), where we're applying another selector to find any +<a>+ (which is a hyperlink) within a +<h4>+ (a title). Here we only want to look at a single element (there's just one title per row), so we have to +pop+ it off the top of the list returned by our selector with the +.pop()+ function.

Note that some elements in the DOM contain actual text (i.e., text that is not part of any markup language), which we can access using the +[element].text+ syntax seen on line 8. Finally, in line 9, we're printing that text to the ScraperWiki console. If you hit run in your scraper, the smaller window should now start listing the event's names from the IAEA website.

[[FIG046]]
.A scraper in action (ScraperWiki)
image::figs/incoming/04-DD.png[scale="90",float="none"]

++++
<?dbfo-need height="1in"?>
++++

You can now see a basic scraper operating: it downloads the web page, transforms it into the DOM form, and then allows you to pick and extract certain content. Given this skeleton, you can try and solve some of the remaining problems using the ScraperWiki and Python documentation:

  * Can you find the address for the link in each event's title?
  * Can you select the small box that contains the date and place by using its CSS class name and extract the element's text?
  * ScraperWiki offers a small database to each scraper so you can store the results; copy the relevant example from their docs and adapt it so it will save the event titles, links and dates.
  * The event list has many pages; can you scrape multiple pages to get historic events as well?

As you're trying to solve these challenges, have a look around ScraperWiki: there are many useful examples in the existing scrapers; quite often, the data is pretty exciting, too. This way, you don't need to start off your scraper from scratch: just choose one that is similar, fork it, and adapt it to your problem.

&mdash; _Friedrich Lindenberg, Open Knowledge Foundation_

.Scraping a Public Database
****
Some French physicians are free to choose their own rates, so that one can pay between €70 and €500 for a 30-minute visit at an oncologist, for instance. This data regarding rates is legally public, but the administration provides only a hard-to-navigate online database. In order to have a good view of the doctors' rates for Le Monde, I decided to scrape the entire database.

That's where the fun began. The front-end search form was a Flash application that redirected to an HTML result page via a POST request. With help from Nicolas Kayser-Bril, it took us some time to figure out that the application used a third page as a ``hidden'' step between the search form and the result page. This page was actually used to store a cookie with values from the search form that was then accessed by the results page. It would have been hard to think of a more convoluted process, but the options of the cURL library in PHP make it easy to overcome the hurdles, once you know where they are! In the end, getting hold of the database was a 10-hour task, but it was worth it.

&mdash; _Alexandre Léchenet, Le Monde_
****

++++
<?dbfo-need height="1in"?>
++++

=== The Web as a Data Source ===

How can you find out more about something that only exists on the Internet? Whether you're looking at an email address, website, image, or Wikipedia article, in this chapter I'll take you through the tools that will tell you more about their backgrounds.

==== Web Tools

First, a few different services you can use to discover more about an entire site, rather than a particular page:

Whois::
  If you go to http://whois.domaintools.com/[whois.domaintools.com] (or just type whois _www.example.com_ in Terminal.app on a Mac, with a URL in place of the placeholder here) you can get the basic registration information for any website. In recent years, some owners have chosen private registration, which hides their details from view, but in many cases you'll see a name, address, email, and phone number for the person who registered the site. You can also enter numerical IP addresses here and get data on the organization or individual that owns that server. This is especially handy when you're trying to track down more information on an abusive or malicious user of a service, since most websites record an IP address for everyone who accesses them.

Blekko::
  The http://blekko.com/[Blekko search engine] offers an unusual amount of insight into the internal statistics it gathers on sites as it crawls the Web. If you type in a domain name followed by "/seo", you'll receive a page of information on that URL.
  The first tab in <<FIG048>> shows you which other sites are linking to the domain in popularity order. This can be extremely useful when you're trying to understand what coverage a site is receiving, and if you want to understand why it's ranking highly in Google's search results, since they're based on those inbound links. <<FIG049>> tells you which other websites are running from the same machine. It's common for scammers and spammers to astroturf their way towards legitimacy by building multiple sites that review and link to each other. They look like independent domains, and may even have different registration details, but they'll often actually live on the same server because that's a lot cheaper. These statistics give you an insight into the hidden business structure of the site you're researching.

[[FIG047]]
.The Blekko search engine (Blekko.com)
image::figs/incoming/06-PP-01.png[float="none"]

[[FIG048]]
.Understanding web popularity: who links to who? The other handy tab is "Crawl stats", especially the "Cohosted with" section. (Blekko.com)
image::figs/incoming/06-PP-02.png[float="none"]

[[FIG049]]
.Spotting web spammers and scammers (Blekko.com)
image::figs/incoming/06-PP-03.png[float="none"]

Compete.com::
  By surveying a cross-section of American consumers, http://www.compete.com/[compete.com] builds up detailed usage statistics for most websites, and makes some basic details freely available. Choose the Site Profile tab and enter a domain (<<FIG0410>>). You'll then see a graph of the site's traffic over the last year, together with figures for how many people visited, and how often (as in <<FIG0411>>). Since they're based on surveys, the numbers are only approximate, but I've found them reasonably accurate when I've been able to compare them against internal analytics. In particular, they seem to be a good source when comparing two sites, since while the absolute numbers may be off for both, it's still a good representation of their relative difference in popularity. They only survey US consumers though, so the data will be poor for predominantly international sites.

[[FIG0410]]
.Compete.com's site profile service (Compete.com)
image::figs/incoming/06-PP-04.png[float="none"]

[[FIG0411]]
.What's in vogue? What's in demand?: Hotspots on the web (Compete.com)
image::figs/incoming/06-PP-05.png[float="none"]

Google's Site Search::
  One feature that can be extremely useful when you're trying to explore all the content on a particular domain is the "site:" keyword. If you add "site:example.com" to your search phrase, Google will only return results from the site you've specified. You can even narrow it down further by including the prefix of the pages you're interested in, for example, "site:example.com/pages/", and you'll only see results that match that pattern. This can be extremely useful when you're trying to find information that domain owners may have made publicly available but aren't keen to publicize, so picking the right keywords can uncover some very revealing material.

==== Web Pages, Images, and Videos

Sometimes you're interested in the activity that's surrounding a particular story, rather than an entire website. The tools below give you different angles on how people are reading, responding to, copying, and sharing content on the web.

Bit.ly::
  I always turn to http://bitly.com/[bit.ly] when I want to know how people are sharing a particular link with each other. To use it, enter the URL you're interested in. Then click on the Info Page+ link. That takes you to the full statistics page (though you may need to choose "aggregrate bit.ly link" first if you're signed in to the service). This will give you an idea of how popular the page is, including activity on Facebook and Twitter, and below that you'll see public conversations about the link provided by backtype.com. I find this combination of traffic data and conversations very helpful when I'm trying to understand why a site or page is popular, and who exactly its fans are. For example, it provided me with strong evidence that the prevailing narrative about grassroots sharing and Sarah Palin was wrong.

++++
<?dbfo-need height="1in"?>
++++

Twitter::
  As the micro-blogging service becomes more widely used, it becomes more useful as a gauge of how people are sharing and talking about individual pieces of content. It's deceptively simple to discover public conversations about a link. You just paste the URL you're interested in into the search box, and then possibly hit "more tweets" to see the full set of results.

Google's Cache::
  When a page becomes controversial, the publishers may take it down or alter it without acknowledgment. If you suspect you're running into the problem, the first place to turn is Google's cache of the page as it was when it did its last crawl. The frequency of crawls is constantly increasing, so you'll have the most luck if you try this within a few hours of the suspected changes. Enter the target URL in Google's search box, and then click the triple arrow on the right of the result for that page. A graphical preview should appear, and if you're lucky, there will be a small "Cache" link at the top of it. Click that to see Google's snapshot of the page. If that has trouble loading, you can switch over to the more primitive text-only page by clicking another link at the top of the full cache page. You'll want to take a screenshot or copy-paste any relevant content you do find, since it may be invalidated at any time by a subsequent crawl.

The Internet Archive's Wayback Machine::
  If you need to know how a particular page has changed over a longer time period, like months or years, the Internet Archive runs a service called http://archive.org/web/web.php[The Wayback Machine] that periodically takes snapshots of the most popular pages on the web. You go to the site, enter the link you want to research, and if it has any copies, it will show you a calendar so you can pick the time you'd like to examine. It will then present a version of the page roughly as it was at that point. It will often be missing styling or images, but it's usually enough to understand what the focus of that page's content was then.

View Source::
  It's a bit of a long shot, but developers often leave comments or other clues in the HTML code that underlies any page. It will be on different menus depending on your browser, but there's always a "View source" option that will let you browse the raw HTML. You don't need to understand what the machine-readable parts mean, just keep an eye out for the pieces of text that are often scattered amongst them. Even if they're just copyright notices or mentions of the author's names, these can often give important clues about the creation and purpose of the page.

TinEye::
  Sometimes you really want to know the source of an image, but without clear attribution text there's no obvious way to do this with traditional search engines like Google. http://www.tineye.com/[TinEye] offers a specialized "reverse image search" process, where you give it the image you have, and it finds other pictures on the web that look very similar. Because they use image recognition to do the matching, it even works when a copy has been cropped, distorted, or compressed. This can be extremely effective when you suspect that an image that's being passed off as original or new is being misrepresented, since it can lead back to the actual source.

YouTube::
  If you click on the Statistics icon to the lower right of any video, you can get a rich set of information about its audience over time. While it's not complete, it is useful for understanding roughly who the viewers are, where they are coming from, and when.

==== Emails
If you have some emails that you're researching, you'll often want to know more details about the sender's identity and location. There isn't a good off-the-shelf tool available to help with this, but it can be very helpful to know the basics about the hidden headers included in every email message. These work like postmarks, and can reveal a surprising amount about the sender. In particular, they often include the IP address of the machine that the email was sent from, a lot like caller ID on a phone call. You can then run whois on that IP number to find out which organization owns that machine. If it turns out to be someone like Comcast or AT&T who provide connections to consumers, then you can visit MaxMind to get its approximate location.

To view these headers in Gmail, open the message and open the menu next to reply on the top right and choose "Show original".

You'll then see a new page revealing the hidden content. There will be a couple of dozen lines at the start that are words followed by a colon. The IP address you're after may be in one of these, but its name will depend on how the email was sent. If it was from Hotmail, it will be called +X-Originating-IP:+, but if it's from Outlook or Yahoo it will be in the first line starting with +Received:+.

Running the address through Whois tells me it's assigned to Virgin Media, an ISP in the UK, so I put it through MaxMind's geolocation service to discover it's coming from my home town of Cambridge. That means I can be reasonably confident this is actually my parents emailing me, not impostors!

==== Trends
If you're digging into a broad topic rather than a particular site or item, here's a couple of tools that can give you some insight:

Wikipedia Article Traffic::
  If you're interested in knowing how public interest in a topic or person has varied over time, you can actually get day-by-day viewing figures for any page on Wikipedia at http://stats.grok.se/[stats.grok.se]. This site is a bit rough and ready, but will let you uncover the information you need with a bit of digging. Enter the name you're interested in to get a monthly view of the traffic on that page. That will bring up a graph showing how many times the page was viewed for each day in the month you specify. Unfortunately you can only see one month at a time, so you'll have to select a new month and search again to see longer-term changes.

Google Insights::
  You can get a clear view into the public's search habits using http://www.google.com/insights/search/[Insights from Google] (<<FIG0412>>). Enter a couple of common search phrases, like "Justin Bieber vs Lady Gaga", and you'll see a graph of their relative number of searches over time. There's a lot of options for refining your view of the data, from narrower geographic areas, to more detail over time. The only disappointment is the lack of absolute values--you only get relative percentages, which can be hard to interpret.

[[FIG0412]]
.Google Insights (Google) 
image::figs/incoming/06-PP-06.png[float="none"]

&mdash; _Pete Warden, independent data analyst and developer_

=== Crowdsourcing Data at the Guardian Datablog

Crowdsourcing, http://en.wikipedia.org/wiki/Crowdsourcing[according to Wikipedia], is "a distributed problem-solving and production process that involves outsourcing tasks to a network of people, also known as the crowd." The following is from an interview with Simon Rogers on how the Datablog used crowdsourcing to cover the MPs' expenses scandal, drug use, and the Sarah Palin papers:

Sometimes you will get a ton of files, statistics, or reports which it is impossible for one person to go through. Also you may get hold of material that is inaccessible or in a bad format and you aren't able to do much with it. This is where crowdsourcing can help.

One thing the Guardian has got is lots of readers, lots of pairs of eyes. If there is an interesting project where we need input, then we can ask them to help us. That is what we did with the http://mps-expenses.guardian.co.uk/[MPs' Expenses]. We had 450,000 documents and very little time to do anything. So what better way than open up the task to our readership?

[[FIG0413]]
.A redacted copy of Stephen Pound's incidental expenses (the Guardian)
image::figs/incoming/04-EE.png[float="none"]

The MPs' Expenses project generated lots of tip-offs. We got more stories than data. The project was remarkably successful in terms of traffic. People really liked it.

We are currently http://bit.ly/guardian-drugs[doing something with MixMag on drug use], which has been phenomenal as well. It looks like it is going to be bigger than the British crime survey in terms of how many people come back to it, which is brilliant. 

What both of these projects have in common is that they are about issues that people really care about, so they are willing to spend time on them. A lot of the crowdsourcing we have done relies on help from obsessives. With the MPs' expenses, we had a massive amount of traffic at the beginning and it really died down. But we still have people that are obsessively going through every page looking for anomalies and stories. One person has done 30,000 pages. They know a lot of stuff.

We also used crowdsourcing with the http://bit.ly/guardian-palin-papers[Sarah Palin papers]. Again this was a great help in scouring the raw information for stories. 

In terms of generating stories crowdsourcing has worked really well for us. People really liked it and it made the Guardian look good. But in terms of generating data, we haven't used crowdsourcing so much.

Some of the crowdsourcing projects that we've done that have worked really well have been more like old-fashioned surveys. When you are asking people about their experience, about their lives, about what they've done, they work very well because people aren't as likely to make that up. They will say what they feel. When we asked people to kind of do our job for us, you have to find a framework for people to produce the data in a way you can trust them.

Regarding the reliability of data, I think the approach that http://www.oldweather.org/[Old Weather] have got is really good. They get ten people to do each entry, which is a good way to ensure accuracy. With the MPs' expenses, we tried to minimize the risk of MPs going online and editing their own records to make themselves look better. But you can't permanently guard against this. You can only really look out for certain URLs or if it's coming from the SW1 area of London. So that's a bit trickier. The data we were getting out was not always reliable. Even though stories were great, it wasn't producing raw numbers that we could confidently use.

If I were to give advice to aspiring data journalists who want to use crowdsourcing to collect data, I would encourage them do this on something that people really care about, and will continue to care about when it stops making front page headlines. Also if you make something more like a game, this can really help to engage people. When we did the expenses story a second time, it was much more like a game with individual tasks for people to do. It really helped to give people specific tasks. That made a big difference because I think if you just present people with the mountain of information to go through and say "go through this," it can make for hard and rather unrewarding work. So I think making it fun is really important.

&mdash; _Marianne Bouchart, Data Journalism Blog, interviewing Simon Rogers, the Guardian_

=== How the Datablog Used Crowdsourcing to Cover Olympic Ticketing

I think the crowdsourcing project that got the biggest response was a http://bit.ly/guardian-olympics[piece on the Olympic ticket ballot]. Thousands of people in the UK tried to get tickets for the 2012 Olympics and there was a lot of fury that people hadn't received them. People had ordered hundreds of pounds worth and were told that they'd get nothing. But no one really knew if it was just some people complaining quite loudly while actually most people were happy. So we tried to work out a way to find out.

We decided the best thing we could really do, with the absence of any good data on the topic, was to ask people. And we thought we'd have to treat it as a light thing because it wasn't a balanced sample. 

We created a Google form and http://bit.ly/guardian-olympics2[asked very specific questions]. It was actually a long form: it asked how much in value people had ordered their tickets, how much their card had been debited for, which events they went for, this kind of thing.

[[FIG0414]]
.How many Olympic tickets did you get?: the readers' results (the Guardian)
image::figs/incoming/04-FF.png[float="0"]

We put it up as a small picture on the front of the site and it was shared around really rapidly. I think this is one of the key things; you can't just think "What do I want to know for my story?", you have to think "What do people want to tell me right now?" And it's only when you tap into what people want to talk about that crowdsourcing is going to be successful. The volume of responses for this project, which is one of our first attempts at crowdsourcing, was huge. We had a thousand responses in less than an hour and seven thousand by the end of that day.

So obviously, we took presenting the results a bit more seriously at this point. Initially, we had no idea how well it would do. So we added some caveats: Guardian readers may be more wealthy than other people, people who got less than they expected might be more willing to talk to us, and so on.

We didn't know how much value the results would have. We ended up having a good seven thousand records to base our piece on, and we found that about half the people who'd asked for tickets had got nothing. We ran all of this stuff and because so many people had taken part the day before, there was a lot of interest in the results.

A few weeks later, the official summary report came out, and our numbers were shockingly close. They were almost exactly spot-on. I think partly through luck, but also because we got just so many people to respond.

If you start asking your readers about something like this on a comments thread, you will be limited in what you can do with the results. So you have to start by thinking, "What is the best tool for what I want to know?" Is it a comment thread? Or is it building an app? And if it is building an app, you have to think "Is this worth the wait? And is it worth the resources that are required to do it?"

In this case, we thought of Google Forms. If someone fills in the form, you can see the result as a row on a spreadsheet. This meant that even if it was still updating, even if results were still coming in, I could open up the spreadsheet and see all of the results straight away.

I could have tried to do the work in Google, but I downloaded it into Microsoft Excel and then did things like sort it from low to high; I also found the entries where people had written out numbers (instead of putting digits) for how much they spent, and fixed all of those. I decided to exclude as little as I could. So rather than taking only valid responses, I tried to fix what I had. Some people had used foreign currencies, so I converted them to sterling, all of which was a bit painstaking.

But the whole analysis was done in a few hours, and I knocked out the obviously silly entries. A lot of people decided to point out that they spent nothing on tickets. That's a bit facetious, but fine. That was less than a hundred out of over seven thousand entries.

Then there were a few dozen who put in obviously fake high amounts to try to distort the results. Things like ten million pounds. So that left me with a set that I could use with the normal data principles we use every day. I did what's called a "pivot table." I did some averaging. That kind of thing.

We didn't have any idea how much momentum the project would have, so it was just me working with the Sports blog editor. We put our heads together and thought this might be a fun project. We did it, start to finish, in 24 hours. We had the idea, we put something up at lunchtime, we put it on the front of the site, we saw it was proving quite popular, we kept it on the front of the site for the rest of the day, and we presented the results online the next morning.

We decided to use Google Docs because it gives complete control over the results. I didn't have to use anyone else's analytic tools. I can put it easily into a database software or into spreadsheets. When you start using specialist polling software, you are often restricted to using their tools. If the information we'd been asking for was particularly sensitive, we might have hesitated before using Google and thought about doing something "in-house." But generally, it is very easy to drop a Google Form into a Guardian page and it's virtually invisible to the user that we are using one. So it is very convenient.

In terms of advice for data journalists who want to use crowdsourcing, you have to have very specific things you want to know. Ask things that get multiple choice responses as much as possible. Try to get some basic demographics of who you are talking to so you can see if your sample might be biased. If you are asking for amounts and things like this, try in the guidance to specify that it's in digits, that they have to use a specific currency, and things like that. A lot won't, but the more you hold their hand throughout, the better. And always, always, add a comment box because a lot of people will fill out the other fields but what they really want is to give you their opinion on the story. Especially on a consumer story or an outrage.

&mdash; _Marianne Bouchart, Data Journalism Blog, interviewing James Ball, the Guardian_

=== Using and Sharing Data: the Black Letter, the Fine Print, and Reality

In this section we'll have a quick look at the state of the law with respect to data and databases, and what you can do to open up your data using readily available public licenses and legal tools. Don't let any of the following dampen your enthusiasm for data-driven journalism. Legal restrictions on data usually won't get in your way, and you can easily make sure they won't get in the way of others using data you've published.

To state the obvious, obtaining data has never been easier. Before the widespread publishing of data on the Web, even if you had identified a dataset you needed, you'd need to ask whoever had a copy to make it accessible to you, possibly involving paper and the post or a personal visit. Now, you have your computer ask their computer to send a copy to your computer. Conceptually similar, but you have a copy right now, and they (the creator or publisher) haven't done anything, and probably have no idea that you have downloaded a copy.
 
What about downloading data with a program (sometimes called ``scraping'') and terms of service (ToS)? Consider the previous paragraph: your browser is just such a program. Might ToS permit access by only certain kinds of programs? If you have inordinate amounts of time and money to spend reading such documents and perhaps asking a lawyer for advice, by all means, do. But usually, just don't be a jerk: if your program hammers a site, your network may well get blocked from accessing the site in question--and perhaps you will have deserved it. There is now a large body of practice around accessing and scraping data from the web. If you plan to do this, reading about examples at a site like ScraperWiki will give you a head start.
 
Once you have some data of interest, you can query, pore over, sort, visualize, correlate, and perform any other kind of analysis you like using your copy of the data. You can publish your analysis, which can cite any data. There's a lot to the catchphrase ``facts are free'' (as in free speech), but maybe this is only a catchphrase among those who think too much about the legalities of databases, or even more broadly (and more wonkily), data governance.

What if, being a good or aspiring-to-be-good data-driven journalist, you intend to publish not just your analysis, including some facts or data points, but also the datasets/databases you used--and perhaps added to--in conducting your analysis? Or maybe you're just curating data and haven't done any analysis (good: the world needs data curators). If you're using data collected by some other entity, there could be a hitch. (If your database is wholly assembled by you, read the next paragraph anyway as motivation for the sharing practices in the next next paragraph.)

If you're familiar with how copyright restricts creative works--if the copyright holder hasn't given permission to use a work (or the work is in the public domain or your use might be covered by exceptions and limitations such as fair use) and you use--distribute, perform, etc.&mdash;the work anyway, the copyright holder could force you to stop. Although facts are free, collections of facts can be restricted very similarly, though there's more variation in the relevant laws than there is for copyright as applied to creative works. Briefly, a database can be subject to copyright, as a creative work. In many jurisdictions, by the ``sweat of the brow,'' merely assembling a database, even in an uncreative fashion, makes the database subject to copyright. In the United States in particular, there tends to be a higher minimum of creativity for copyright to apply (Feist v. Rural, a case about a phone book, is the U.S. classic if you want to look it up). But in some jurisdictions there are also ``database rights'' that restrict databases, separate from copyright (though there is lots of overlap in terms of what is covered, in particular where creativity thresholds for copyright are nearly nonexistent). The best known of such are the European Union's _sui generis_ database rights. Again, especially if you're in Europe, you may want to make sure you have permission before publishing a database from some other entity.

Obviously such restrictions aren't the best way to grow an ecosystem of data-driven journalism (nor are they good for society at large--social scientists and others told the EU they wouldn't be before _sui generis_ came about, and studies since have shown them to be right). Fortunately, as a publisher of a database, you can remove such restrictions from the database (assuming it doesn't have elements that you don't have permission to grant further permissions around), essentially by granting permission in advance. You can do this by releasing your database under a public license or public domain dedication--just as many programmers release their code under a free and open source license, so that others can build on their code (as data-driven journalism often involves code, not just data, of course you should release your code too, so that your data collection and analysis are reproducible). There are lots of reasons for opening up your data. For example, your audience might create new visualizations or applications with it that you can link to--as the Guardian does with their data visualization Flickr pool. Your datasets can be combined with other datasets to give you and your readers greater insight into a topic. Things that others do with your data might give you leads for new stories, or ideas for stories, or ideas for other data-driven projects. And they will certainly bring you kudos.

[[FIG0415]]
.Open Data badges (Open Knowledge Foundation)
image::figs/incoming/04-GG.jpg[float="none"]

When one realizes that releasing works under public licenses is a necessity, the question becomes, which license? That tricky question will frequently be answered by the project or community whose work you're building on, or that you hope to contribute your work to--use the license they use. If you need to dig deeper, start from the set of licenses that are free and open--meaning that anyone has permission, for any use (attribution and sharing alike might be required). What the Free Software Definition and Open Source Definition do for software, the http://opendefinition.org/[Open Knowledge Definition] does for all other knowledge, including databases: define what makes a work open, and what open licenses allow users to do.
 
You can visit the Open Knowledge Definition website to see the http://opendefinition.org/licenses/[current set of licenses which qualify]. In summary, there are basically three classes of open licenses:

Public domain dedications::
  These also serve as maximally permissive licenses; there are no conditions put upon using the work.

Permissive or attribution-only licenses::
  Giving credit is the only substantial condition of these licenses.

Copyleft, reciprocal, or share-alike licenses::
  These also require that modified works, if published, be shared under the same license.

Note if you're using a dataset published by someone else under an open license, consider the above paragraph a very brief guide as to how to fulfill the conditions of that open license. The licenses you're most likely to encounter, from Creative Commons, Open Data Commons, and various governments, usually feature a summary that will easily allow you to see what the substantial conditions are. Typically the license will be noted on a web page from which a dataset may be downloaded (or ``scraped'', as of course, web pages can contain datasets) or in a conspicuous place within the dataset itself, depending on format. This marking is what you should do as well, when you open up your datasets.

Going back to the beginning, what if the dataset you need to obtain is still not available online, or behind a some kind of access control? Consider, in addition to asking for access yourself, requesting that the data to be opened up for the world to reuse. You could give some pointers to some of the great things that can happen with their data if they do this.
 
Sharing with the world might bring to mind that privacy and other considerations and regulations might come into play for some datasets. Indeed, just because open data lowers many technical and copyright and copyright-like barriers, doesn't mean you don't have to follow other applicable laws. But that's as it always was, and there are tremendous resources and sometimes protections for journalists should your common sense indicate a need to investigate those.
 
Good luck! But in all probability you'll need more of it for other areas of your project than you'll need for managing the (low) legal risks.

&mdash; _Mike Linksvayer, Creative Commons_

